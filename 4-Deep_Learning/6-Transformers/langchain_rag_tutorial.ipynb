{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain representa una revolución en el desarrollo de aplicaciones basadas en Inteligencia Artificial, específicamente en la integración y utilización de Modelos de Lenguaje de Gran Escala (LLMs). Este marco de trabajo innovador ha transformado la manera en que desarrolladores y organizaciones implementan soluciones de IA.\n",
    "\n",
    "## Fundamentos y Arquitectura\n",
    "\n",
    "**Conceptos Básicos**\n",
    "LangChain se construye sobre el principio de componibilidad, permitiendo la creación de aplicaciones complejas mediante la combinación de componentes más simples. Su arquitectura modular facilita la integración con diversos modelos de lenguaje, bases de datos y servicios externos.\n",
    "\n",
    "**Componentes Principales**\n",
    "- Prompts: Plantillas y sistemas de gestión para la generación de instrucciones efectivas\n",
    "- Chains: Secuencias de operaciones que combinan diferentes componentes\n",
    "- Agents: Entidades autónomas que pueden tomar decisiones y ejecutar acciones\n",
    "- Memory: Sistemas para mantener el contexto en conversaciones y procesos\n",
    "- Indexes: Estructuras para organizar y acceder a datos de manera eficiente\n",
    "\n",
    "## Casos de Uso Prácticos\n",
    "\n",
    "**Análisis de Documentos**\n",
    "LangChain destaca en el procesamiento y análisis de documentos extensos. Por ejemplo:\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = PyPDFLoader(\"documento.pdf\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "response = index.query(\"¿Cuáles son los puntos principales del documento?\")\n",
    "```\n",
    "\n",
    "**Asistentes Virtuales Personalizados**\n",
    "La creación de chatbots avanzados se simplifica mediante el uso de cadenas conversacionales:\n",
    "\n",
    "```python\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "```\n",
    "\n",
    "## Capacidades Avanzadas\n",
    "\n",
    "**Integración con Bases de Datos**\n",
    "LangChain permite la interacción directa con bases de datos, facilitando consultas en lenguaje natural:\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=SQLDatabaseToolkit(db=db),\n",
    "    verbose=True\n",
    ")\n",
    "```\n",
    "\n",
    "**Procesamiento de Conocimiento**\n",
    "El framework sobresale en la gestión y utilización de bases de conocimiento:\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "```\n",
    "\n",
    "## Aplicaciones Empresariales\n",
    "\n",
    "**Automatización de Procesos**\n",
    "- Análisis automático de contratos y documentos legales\n",
    "- Generación de informes y resúmenes ejecutivos\n",
    "- Clasificación y routing de correos electrónicos\n",
    "- Extracción de información de documentos no estructurados\n",
    "\n",
    "**Interacción con Clientes**\n",
    "- Sistemas de atención al cliente 24/7\n",
    "- Asistentes virtuales para ventas\n",
    "- Sistemas de recomendación personalizados\n",
    "- Chatbots multilingües\n",
    "\n",
    "## Características Técnicas Avanzadas\n",
    "\n",
    "**Gestión de Memoria**\n",
    "LangChain implementa diversos tipos de memoria:\n",
    "- Buffer Memory: Almacenamiento simple de mensajes\n",
    "- Summary Memory: Resúmenes de conversaciones anteriores\n",
    "- Vector Memory: Almacenamiento basado en embeddings\n",
    "\n",
    "**Herramientas de Desarrollo**\n",
    "- Debugging avanzado de cadenas y agentes\n",
    "- Monitorización de tokens y costos\n",
    "- Sistemas de logging y trazabilidad\n",
    "- Herramientas de testing y evaluación\n",
    "\n",
    "## Integración y Escalabilidad\n",
    "\n",
    "**Compatibilidad con Proveedores**\n",
    "LangChain se integra con múltiples proveedores de LLMs:\n",
    "- OpenAI (GPT-3.5, GPT-4)\n",
    "- Anthropic (Claude)\n",
    "- Google (PaLM)\n",
    "- Hugging Face (modelos open source)\n",
    "\n",
    "**Optimización de Recursos**\n",
    "- Caching inteligente de respuestas\n",
    "- Gestión eficiente de tokens\n",
    "- Balanceo de carga entre diferentes modelos\n",
    "- Estrategias de fallback\n",
    "\n",
    "## Consideraciones de Implementación\n",
    "\n",
    "**Mejores Prácticas**\n",
    "- Diseño modular de cadenas y agentes\n",
    "- Implementación de sistemas de retry y error handling\n",
    "- Monitorización de costos y uso de recursos\n",
    "- Validación y testing de prompts\n",
    "\n",
    "**Seguridad y Privacidad**\n",
    "- Manejo seguro de credenciales\n",
    "- Sanitización de inputs\n",
    "- Control de acceso granular\n",
    "- Cumplimiento de normativas de privacidad\n",
    "\n",
    "## Futuro y Evolución\n",
    "\n",
    "LangChain continúa evolucionando rápidamente, con nuevas características y mejoras constantes:\n",
    "- Soporte para modelos multimodales\n",
    "- Mejoras en la eficiencia de procesamiento\n",
    "- Nuevas integraciones con herramientas y servicios\n",
    "- Expansión de capacidades de razonamiento\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "LangChain representa un salto cualitativo en el desarrollo de aplicaciones de IA, proporcionando una infraestructura robusta y flexible para la creación de soluciones sofisticadas. Su arquitectura modular, junto con su amplia gama de funcionalidades, lo convierte en una herramienta indispensable para cualquier desarrollador o organización que busque implementar soluciones basadas en LLMs.\n",
    "\n",
    "La combinación de su facilidad de uso, potencia y flexibilidad hace de LangChain una elección óptima para proyectos que requieren procesamiento avanzado de lenguaje natural, desde simples chatbots hasta complejos sistemas de análisis y automatización empresarial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 0.\n",
    "\n",
    "Obtener los siguientes API Keys (Ambos son gratuitos).\n",
    "\n",
    "- API Key de Google AI Studio Para acceder al modelo de Gemini de Google.\n",
    "- API Key de Langchain\n",
    "\n",
    "## Paso 1. Instalación de Librerías y utilitarios\n",
    "\n",
    "Instalamos las librerías necesarias y las importamos en el notebook/script que utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade -q langchain\n",
    "# !pip install google-generativeai langchain-google-genai\n",
    "# !pip install chromadb pypdf2 python-dotenv\n",
    "# !pip install PyPDF\n",
    "# !pip install -U langchain-community\n",
    "# !pip install sentence-transformers\n",
    "# !pip install langchainhub    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several options, depending on the desired tone and style:\n",
      "\n",
      "**Option 1 (Simple & Sweet):**\n",
      "\n",
      "```flux\n",
      "(let ((message \"Season's Greetings! Wishing you a warm and joyful holiday season.\"))\n",
      "  (layout (text message :font \"Arial\" :size 24 :color \"darkred\")\n",
      "          (image \"christmas-tree.jpg\" :width 100 :height 100)\n",
      "          :margin 20))\n",
      "```\n",
      "This assumes you have a `christmas-tree.jpg` image file.  You'll need to adjust paths as needed.\n",
      "\n",
      "\n",
      "**Option 2 (More Complex with Animation):**\n",
      "\n",
      "```flux\n",
      "(let ((snowflakes (repeat 20 (circle 5 :fill \"white\" :opacity 0.8)))\n",
      "      (message \"Merry Christmas & Happy New Year!\"))\n",
      "  (animate (interval 0.1 (fn []\n",
      "                            (for [snowflake snowflakes]\n",
      "                              (set! (:y snowflake) (+ (:y snowflake) 2)\n",
      "                                    (:opacity snowflake) (max 0 (- (:opacity snowflake) 0.01))))\n",
      "                        (when (> (:y (first snowflakes)) 400) ; Adjust 400 for card height\n",
      "                          (doseq [snowflake snowflakes]\n",
      "                            (set! (:y snowflake) -10 ; Reset snowflake position\n",
      "                                  (:opacity snowflake) 0.8))))\n",
      "         (layout (text message :font \"Times New Roman\" :size 30 :color \"green\" :x 50 :y 50)\n",
      "                 snowflakes\n",
      "                 :width 500 :height 400)))\n",
      "```\n",
      "This creates falling snowflakes.  You'll need to adjust numbers for size and speed.\n",
      "\n",
      "\n",
      "**Option 3 (Personalized):**\n",
      "\n",
      "```flux\n",
      "(let ((name \"Recipient Name\")\n",
      "      (message (str \"Dear \" name \",\\nWishing you a very Merry Christmas and a happy New Year!\\n\\nBest,\\nYour Name\")))\n",
      "  (layout (text message :font \"Georgia\" :size 18 :color \"navy\" :wrap true :width 300)\n",
      "          (image \"personal-image.jpg\" :width 150 :height 100)\n",
      "          :margin 30))\n",
      "```\n",
      "Again, adjust paths and sizes as needed.  Remember to replace `\"Recipient Name\"` and `\"Your Name\"` and `\"personal-image.jpg\"` with appropriate values.\n",
      "\n",
      "\n",
      "Remember to replace placeholder image filenames with actual image files.  You'll need a Flux environment set up to run this code.  These examples provide a basic framework; you can expand upon them with additional features and styling as desired.  The specific capabilities will depend on your Flux version and any additional libraries you've incorporated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_CUSTOM_SEARCH_API\"])\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"Generate a prompt for a christmas card in flux.1\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"cold\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.047469958662986755\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 6,\n",
       "        \"candidates_token_count\": 2,\n",
       "        \"total_token_count\": 8\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Librerías para la preparación de datos\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Librerías para el proceso de Retrieval\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2. Configuración inicial de carpetas\n",
    "\n",
    "Crearemos dos carpetas:\n",
    "\n",
    "- MisDatos: Aquí se almacenarán los archivos adicionales que utilizaremos para ampliar la base de conocimiento del modelo.\n",
    "- VectorDB: En esta carpeta se almacenará la base de datos Vectorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.mkdir(\"content\")\n",
    "# os.mkdir(\"content/MisDatos\")\n",
    "# os.mkdir(\"content/VectorDB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga en la carpeta MisDatos, los PDFs que quieres utilizar para personalizar las respuestas generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3. Preparación de los datos\n",
    "\n",
    "Leeremos los archivos PDF de la carpeta Mis Datos y los convertiremos en embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.lostiempos.com/sites/default/files/edicion_online/las_delicias_de_mi_llajta.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data_folder = \"./content/MisDatos\"\n",
    "# Leyendo los PDFs del directorio configurado\n",
    "loader = PyPDFDirectoryLoader(source_data_folder)\n",
    "data_on_pdf = loader.load()\n",
    "# cantidad de data cargada\n",
    "len(data_on_pdf)\n",
    "\n",
    "\n",
    "# Particionando los datos. Con un tamaño delimitado (chunks) y \n",
    "# 200 caracters de overlapping para preservar el contexto\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "splits = text_splitter.split_documents(data_on_pdf)\n",
    "# Cantidad de chunks obtenidos\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = splits[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1\n",
      "Guía de \n",
      "Frutos Silvestres Comestibles \n",
      "de la Chiquitania\n",
      "Diego Javier Coimbra Molina' metadata={'source': 'content\\\\MisDatos\\\\chiquitania.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_on_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a generar los embeddings y almacenarlos en Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "# Crea la instancia de embeddings con Cohere\n",
    "embeddings_model = CohereEmbeddings(cohere_api_key=os.environ[\"COHERE_API_KEY\"], user_agent=\"Fran\")\n",
    "\n",
    "path_db = \"./content/VectorDB\"  # Ruta a la base de datos del vector store\n",
    "\n",
    "# Crear el vector store a partir de tus documentos 'splits'\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=embeddings_model, \n",
    "    # persist_directory=path_db\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Configuración del Retrieval\n",
    "\n",
    "El LLM que usaremos para estructurar las respuestas será Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=os.environ[\"GOOGLE_CUSTOM_SEARCH_API\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fuente para el retriever será la base de datos de Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Any'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'VectorStoreRetriever'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSource:\u001b[0m   \n",
      "    \u001b[1;32mdef\u001b[0m \u001b[0mas_retriever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVectorStoreRetriever\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;34m\"\"\"Return VectorStoreRetriever initialized from this VectorStore.\n",
      "\n",
      "        Args:\n",
      "            **kwargs: Keyword arguments to pass to the search function.\n",
      "                Can include:\n",
      "                search_type (Optional[str]): Defines the type of search that\n",
      "                    the Retriever should perform.\n",
      "                    Can be \"similarity\" (default), \"mmr\", or\n",
      "                    \"similarity_score_threshold\".\n",
      "                search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      "                    search function. Can include things like:\n",
      "                        k: Amount of documents to return (Default: 4)\n",
      "                        score_threshold: Minimum relevance threshold\n",
      "                            for similarity_score_threshold\n",
      "                        fetch_k: Amount of documents to pass to MMR algorithm\n",
      "                            (Default: 20)\n",
      "                        lambda_mult: Diversity of results returned by MMR;\n",
      "                            1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      "                        filter: Filter by document metadata\n",
      "\n",
      "        Returns:\n",
      "            VectorStoreRetriever: Retriever class for VectorStore.\n",
      "\n",
      "        Examples:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            # Retrieve more documents with higher diversity\n",
      "            # Useful if your dataset has many similar documents\n",
      "            docsearch.as_retriever(\n",
      "                search_type=\"mmr\",\n",
      "                search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      "            )\n",
      "\n",
      "            # Fetch more documents for the MMR algorithm to consider\n",
      "            # But only return the top 5\n",
      "            docsearch.as_retriever(\n",
      "                search_type=\"mmr\",\n",
      "                search_kwargs={'k': 5, 'fetch_k': 50}\n",
      "            )\n",
      "\n",
      "            # Only retrieve documents that have a relevance score\n",
      "            # Above a certain threshold\n",
      "            docsearch.as_retriever(\n",
      "                search_type=\"similarity_score_threshold\",\n",
      "                search_kwargs={'score_threshold': 0.8}\n",
      "            )\n",
      "\n",
      "            # Only get the single most similar document from the dataset\n",
      "            docsearch.as_retriever(search_kwargs={'k': 1})\n",
      "\n",
      "            # Use a filter to only retrieve documents from a specific paper\n",
      "            docsearch.as_retriever(\n",
      "                search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      "            )\n",
      "        \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tags\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_retriever_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mVectorStoreRetriever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\ort\\miniconda3\\envs\\ia_env\\lib\\site-packages\\langchain_core\\vectorstores\\base.py\n",
      "\u001b[1;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "vectorstore.as_retriever??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos uno de los templates de prompts básicos de LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente definimos la variable rag_chain en la que pedimos a Langchain que reciba la pregunta, busque la respuesta en Chroma DB (utilizando el retriever) y solicite a Gemini que genere la respuesta en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    # Funcion auxiliar para enviar el contexto al modelo como parte del prompt\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La pacobilla (Capparidastrum coimbranum) es una fruta nativa chiquitana.  También se le conoce como platanillo en San Ignacio.  Es un árbol con potencial ornamental por su forma, follaje y flores.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Preguntas al Documento\n",
    "pregunta = \"Que es la pacobilla? \" # @param {type:\"string\"}\n",
    "response = rag_chain.invoke(pregunta )\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages[0].prompt.template = \"You are a Bolivian kitcken expert assistant for question-answering tasks (thus you might wanna answer in Spanish). Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A pacobilla is a small Chiquitana fruit. Its scientific name is *Capparidastrum coimbranum*.  The name is a tribute to the author's father.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Preguntas al Documento\n",
    "pregunta = \"What is a pacobilla? \" # @param {type:\"string\"}\n",
    "response = rag_chain.invoke(pregunta )\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde LangSmith también puedes ver el proceso de razonamiento completo cada vez que haces una pregunta usando LangChain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
