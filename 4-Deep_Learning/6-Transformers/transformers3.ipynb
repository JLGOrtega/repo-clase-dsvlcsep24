{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMERS 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"ab ca\"><div class=\"ch bg dx dy dz ea\"><p id=\"d240\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">This is the third article in my series on Transformers. We are covering its functionality in a top-down manner. In the previous articles, we learned what a Transformer is, its architecture, and how it works.</p><p id=\"d3be\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In this article, we will go a step further and dive deeper into Multi-head Attention, which is the brains of the Transformer.</p><p id=\"4dab\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Here’s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.</p><ol class=\"\"><li id=\"51a3\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452\">Overview of functionality</a> <em class=\"oa\">(How Transformers are used, and why they are better than RNNs. Components of the architecture, and behavior during Training and Inference)</em></li><li id=\"03f4\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34\">How it works</a> <em class=\"oa\">(Internal operation end-to-end. How data flows and what computations are performed, including matrix representations)</em></li><li id=\"fee8\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Multi-head Attention — this article</strong> <em class=\"oa\">(Inner workings of the Attention module throughout the Transformer)</em></li><li id=\"b569\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3\">Why Attention Boosts Performance</a> <em class=\"oa\">(Not just what Attention does but why it works so well. How does Attention capture the relationships between words in a sentence)</em></li></ol><p id=\"8681\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">And if you’re interested in NLP applications in general, I have some other articles you might like.</p><ol class=\"\"><li id=\"8c49\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24\">Beam Search</a> <em class=\"oa\">(Algorithm commonly used by Speech-to-Text and NLP applications to enhance predictions)</em></li><li id=\"1c06\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b\">Bleu Score</a> (<em class=\"oa\">Bleu Score and Word Error Rate are two essential metrics for NLP models</em>)</li></ol><h1 id=\"5d8f\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">How Attention is used in the Transformer</h1><p id=\"3e67\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">As we discussed in <a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34\">Part 2</a>, Attention is used in the Transformer in three places:</p><ul class=\"\"><li id=\"9588\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Self-attention in the Encoder — the input sequence pays attention to itself</li><li id=\"1a95\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Self-attention in the Decoder — the target sequence pays attention to itself</li><li id=\"3aa5\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Encoder-Decoder-attention in the Decoder — the target sequence pays attention to the input sequence</li></ul><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pi\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*47UCxMjpfJ2yo48fctNv-g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*47UCxMjpfJ2yo48fctNv-g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*47UCxMjpfJ2yo48fctNv-g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*47UCxMjpfJ2yo48fctNv-g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*47UCxMjpfJ2yo48fctNv-g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*47UCxMjpfJ2yo48fctNv-g.png 1100w, https://miro.medium.com/v2/resize:fit:970/format:webp/1*47UCxMjpfJ2yo48fctNv-g.png 970w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 485px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*47UCxMjpfJ2yo48fctNv-g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*47UCxMjpfJ2yo48fctNv-g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*47UCxMjpfJ2yo48fctNv-g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*47UCxMjpfJ2yo48fctNv-g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*47UCxMjpfJ2yo48fctNv-g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*47UCxMjpfJ2yo48fctNv-g.png 1100w, https://miro.medium.com/v2/resize:fit:970/1*47UCxMjpfJ2yo48fctNv-g.png 970w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 485px\"><img alt=\"\" class=\"bg mq mr c\" width=\"485\" height=\"530\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:728/1*47UCxMjpfJ2yo48fctNv-g.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"f7a2\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Attention Input Parameters — Query, Key, and Value</strong></p><p id=\"9ae1\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Attention layer takes its input in the form of three parameters, known as the Query, Key, and Value.</p><p id=\"1a65\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">All three parameters are similar in structure, with each word in the sequence represented by a vector.</p><p id=\"4f85\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Encoder Self-Attention</strong></p><p id=\"74ec\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The input sequence is fed to the Input Embedding and Position Encoding, which produces an encoded representation for each word in the input sequence that captures the meaning and position of each word. This is fed to all three parameters, Query, Key, and Value in the Self-Attention in the first Encoder which then also produces an encoded representation for each word in the input sequence, that now incorporates the attention scores for each word as well. As this passes through all the Encoders in the stack, each Self-Attention module also adds its own attention scores into each word’s representation.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*OYi49Pkg-Vl3D4HleEuu7g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*OYi49Pkg-Vl3D4HleEuu7g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*OYi49Pkg-Vl3D4HleEuu7g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*OYi49Pkg-Vl3D4HleEuu7g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*OYi49Pkg-Vl3D4HleEuu7g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*OYi49Pkg-Vl3D4HleEuu7g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*OYi49Pkg-Vl3D4HleEuu7g.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"506\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*OYi49Pkg-Vl3D4HleEuu7g.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"3f2d\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Decoder Self-Attention</strong></p><p id=\"92bb\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Coming to the Decoder stack, the target sequence is fed to the Output Embedding and Position Encoding, which produces an encoded representation for each word in the target sequence that captures the meaning and position of each word. This is fed to all three parameters, Query, Key, and Value in the Self-Attention in the first Decoder which then also produces an encoded representation for each word in the target sequence, which now incorporates the attention scores for each word as well.</p><p id=\"6c6d\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">After passing through the Layer Norm, this is fed to the Query parameter in the Encoder-Decoder Attention in the first Decoder</p><p id=\"2870\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Encoder-Decoder Attention</strong></p><p id=\"3674\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Along with that, the output of the final Encoder in the stack is passed to the Value and Key parameters in the Encoder-Decoder Attention.</p><p id=\"95ac\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Encoder-Decoder Attention is therefore getting a representation of both the target sequence (from the Decoder Self-Attention) and a representation of the input sequence (from the Encoder stack). It, therefore, produces a representation with the attention scores for each target sequence word that captures the influence of the attention scores from the input sequence as well.</p><p id=\"d900\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">As this passes through all the Decoders in the stack, each Self-Attention and each Encoder-Decoder Attention also add their own attention scores into each word’s representation.</p><h1 id=\"9c6a\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Multiple Attention Heads</h1><p id=\"50e4\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">In the Transformer, the Attention module repeats its computations multiple times in parallel. Each of these is called an Attention Head. The Attention module splits its Query, Key, and Value parameters N-ways and passes each split independently through a separate Head. All of these similar Attention calculations are then combined together to produce a final Attention score. This is called Multi-head attention and gives the Transformer greater power to encode multiple relationships and nuances for each word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*DKNIOlVfbh9K1EqU5iDJKA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*DKNIOlVfbh9K1EqU5iDJKA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*DKNIOlVfbh9K1EqU5iDJKA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*DKNIOlVfbh9K1EqU5iDJKA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*DKNIOlVfbh9K1EqU5iDJKA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DKNIOlVfbh9K1EqU5iDJKA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DKNIOlVfbh9K1EqU5iDJKA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*DKNIOlVfbh9K1EqU5iDJKA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*DKNIOlVfbh9K1EqU5iDJKA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*DKNIOlVfbh9K1EqU5iDJKA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*DKNIOlVfbh9K1EqU5iDJKA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*DKNIOlVfbh9K1EqU5iDJKA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*DKNIOlVfbh9K1EqU5iDJKA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*DKNIOlVfbh9K1EqU5iDJKA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"564\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*DKNIOlVfbh9K1EqU5iDJKA.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"546b\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">To understand exactly how the data is processed internally, let’s walk through the working of the Attention module while we are training the Transformer to solve a translation problem. We’ll use one sample of our training data which consists of an input sequence (‘You are welcome’ in English) and a target sequence (‘De nada’ in Spanish).</p><h1 id=\"3bdd\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Attention Hyperparameters</h1><p id=\"8af1\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">There are three hyperparameters that determine the data dimensions:</p><ul class=\"\"><li id=\"41e6\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Embedding Size — width of the embedding vector (we use a width of 6 in our example). This dimension is carried forward throughout the Transformer model and hence is sometimes referred to by other names like ‘model size’ etc.</li><li id=\"d62d\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Query Size (equal to Key and Value size)— the size of the weights used by three Linear layers to produce the Query, Key, and Value matrices respectively (we use a Query size of 3 in our example)</li><li id=\"5ddb\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Number of Attention heads (we use 2 heads in our example)</li></ul><p id=\"5a9d\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In addition, we also have the Batch size, giving us one dimension for the number of samples.</p><h1 id=\"4039\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Input Layers</h1><p id=\"aeb8\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The Input Embedding and Position Encoding layers produce a matrix of shape (Number of Samples, Sequence Length, Embedding Size) which is fed to the Query, Key, and Value of the first Encoder in the stack.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pk\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8JpkIFoNmN7CeXSbOd8-9A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*8JpkIFoNmN7CeXSbOd8-9A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*8JpkIFoNmN7CeXSbOd8-9A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*8JpkIFoNmN7CeXSbOd8-9A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*8JpkIFoNmN7CeXSbOd8-9A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*8JpkIFoNmN7CeXSbOd8-9A.png 1100w, https://miro.medium.com/v2/resize:fit:1040/format:webp/1*8JpkIFoNmN7CeXSbOd8-9A.png 1040w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*8JpkIFoNmN7CeXSbOd8-9A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*8JpkIFoNmN7CeXSbOd8-9A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*8JpkIFoNmN7CeXSbOd8-9A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*8JpkIFoNmN7CeXSbOd8-9A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*8JpkIFoNmN7CeXSbOd8-9A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*8JpkIFoNmN7CeXSbOd8-9A.png 1100w, https://miro.medium.com/v2/resize:fit:1040/1*8JpkIFoNmN7CeXSbOd8-9A.png 1040w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px\"><img alt=\"\" class=\"bg mq mr c\" width=\"520\" height=\"430\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:780/1*8JpkIFoNmN7CeXSbOd8-9A.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"02e1\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">To make it simple to visualize, we will drop the Batch dimension in our pictures and focus on the remaining dimensions.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*ONqbeofLVaCYHl5QQnOJCw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ONqbeofLVaCYHl5QQnOJCw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ONqbeofLVaCYHl5QQnOJCw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ONqbeofLVaCYHl5QQnOJCw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ONqbeofLVaCYHl5QQnOJCw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ONqbeofLVaCYHl5QQnOJCw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONqbeofLVaCYHl5QQnOJCw.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*ONqbeofLVaCYHl5QQnOJCw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ONqbeofLVaCYHl5QQnOJCw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ONqbeofLVaCYHl5QQnOJCw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ONqbeofLVaCYHl5QQnOJCw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ONqbeofLVaCYHl5QQnOJCw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ONqbeofLVaCYHl5QQnOJCw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ONqbeofLVaCYHl5QQnOJCw.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"224\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*ONqbeofLVaCYHl5QQnOJCw.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"fe8e\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Linear Layers</h1><p id=\"63f7\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">There are three separate Linear layers for the Query, Key, and Value. Each Linear layer has its own weights. The input is passed through these Linear layers to produce the Q, K, and V matrices.</p></div></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" class=\"bg mq mr c\" width=\"1000\" height=\"447\" loading=\"eager\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1500/1*NR9x97HungXtL6Vx0CGmow.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"ab ca\"><div class=\"ch bg dx dy dz ea\"><h1 id=\"956c\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Splitting data across Attention heads</h1><p id=\"a050\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">Now the data gets split across the multiple Attention heads so that each can process it independently.</p><p id=\"f614\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><mark class=\"adq adr ao\">However, the important thing to understand is that this is a logical split only. The Query, Key, and Value are not physically split into separate matrices, one for each Attention head. A single data matrix is used for the Query, Key, and Value, respectively, with logically separate sections of the matrix for each Attention head. Similarly, there are not separate Linear layers, one for each Attention head. All the Attention heads share the same Linear layer but simply operate on their ‘own’ logical section of the data matrix.</mark></p><p id=\"fa0e\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Linear layer weights are logically partitioned per head</strong></p><p id=\"5e0f\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">This logical split is done by partitioning the input data as well as the Linear layer weights uniformly across the Attention heads. We can achieve this by choosing the Query Size as below:</p><p id=\"05bd\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><em class=\"oa\">Query Size = Embedding Size / Number of heads</em></p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_2mb_TfvIMsZoIFILNq0CA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_2mb_TfvIMsZoIFILNq0CA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_2mb_TfvIMsZoIFILNq0CA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_2mb_TfvIMsZoIFILNq0CA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_2mb_TfvIMsZoIFILNq0CA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_2mb_TfvIMsZoIFILNq0CA.png 1100w, https://miro.medium.com/v2/resize:fit:960/format:webp/1*_2mb_TfvIMsZoIFILNq0CA.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*_2mb_TfvIMsZoIFILNq0CA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_2mb_TfvIMsZoIFILNq0CA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_2mb_TfvIMsZoIFILNq0CA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_2mb_TfvIMsZoIFILNq0CA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_2mb_TfvIMsZoIFILNq0CA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_2mb_TfvIMsZoIFILNq0CA.png 1100w, https://miro.medium.com/v2/resize:fit:960/1*_2mb_TfvIMsZoIFILNq0CA.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\"><img alt=\"\" class=\"bg mq mr c\" width=\"480\" height=\"300\" loading=\"eager\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:720/1*_2mb_TfvIMsZoIFILNq0CA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"6d1b\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In our example, that is why the Query Size = 6/2 = 3. Even though the layer weight (and input data) is a single matrix we can think of it as ‘stacking together’ the separate layer weights for each head.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*_-Na5JQVQH078he-RouboA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_-Na5JQVQH078he-RouboA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_-Na5JQVQH078he-RouboA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_-Na5JQVQH078he-RouboA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_-Na5JQVQH078he-RouboA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_-Na5JQVQH078he-RouboA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-Na5JQVQH078he-RouboA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*_-Na5JQVQH078he-RouboA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_-Na5JQVQH078he-RouboA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_-Na5JQVQH078he-RouboA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_-Na5JQVQH078he-RouboA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_-Na5JQVQH078he-RouboA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_-Na5JQVQH078he-RouboA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_-Na5JQVQH078he-RouboA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"244\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*_-Na5JQVQH078he-RouboA.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"2226\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The computations for all Heads can be therefore be achieved via a single matrix operation rather than requiring N separate operations. This makes the computations more efficient and keeps the model simple because fewer Linear layers are required, while still achieving the power of the independent Attention heads.</p><p id=\"aece\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Reshaping the Q, K, and V matrices</strong></p><p id=\"6d69\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Q, K, and V matrices output by the Linear layers are reshaped to include an explicit Head dimension. Now each ‘slice’ corresponds to a matrix per head.</p><p id=\"11af\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">This matrix is reshaped again by swapping the Head and Sequence dimensions. Although the Batch dimension is not drawn, the dimensions of Q are now (Batch, Head, Sequence, Query size).</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*iHL0mjVjvBkWEZaFqFnohg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iHL0mjVjvBkWEZaFqFnohg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*iHL0mjVjvBkWEZaFqFnohg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*iHL0mjVjvBkWEZaFqFnohg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*iHL0mjVjvBkWEZaFqFnohg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iHL0mjVjvBkWEZaFqFnohg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHL0mjVjvBkWEZaFqFnohg.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*iHL0mjVjvBkWEZaFqFnohg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*iHL0mjVjvBkWEZaFqFnohg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*iHL0mjVjvBkWEZaFqFnohg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*iHL0mjVjvBkWEZaFqFnohg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*iHL0mjVjvBkWEZaFqFnohg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*iHL0mjVjvBkWEZaFqFnohg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*iHL0mjVjvBkWEZaFqFnohg.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"195\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*iHL0mjVjvBkWEZaFqFnohg.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">The Q matrix is reshaped to include a Head dimension and then reshaped again by swapping the Head and Sequencd dimensions. (Image by Author)</figcaption></figure><p id=\"22b5\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In the picture below, we can see the complete process of splitting our example Q matrix, after coming out of the Linear layer.</p><p id=\"cd5f\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The final stage is for visualization only — although the Q matrix is a single matrix, we can think of it as a logically separate Q matrix per head.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*j5j3YvMgOfy4VoEMtuJGVA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*j5j3YvMgOfy4VoEMtuJGVA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*j5j3YvMgOfy4VoEMtuJGVA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*j5j3YvMgOfy4VoEMtuJGVA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*j5j3YvMgOfy4VoEMtuJGVA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*j5j3YvMgOfy4VoEMtuJGVA.png 1100w, https://miro.medium.com/v2/resize:fit:960/format:webp/1*j5j3YvMgOfy4VoEMtuJGVA.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*j5j3YvMgOfy4VoEMtuJGVA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*j5j3YvMgOfy4VoEMtuJGVA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*j5j3YvMgOfy4VoEMtuJGVA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*j5j3YvMgOfy4VoEMtuJGVA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*j5j3YvMgOfy4VoEMtuJGVA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*j5j3YvMgOfy4VoEMtuJGVA.png 1100w, https://miro.medium.com/v2/resize:fit:960/1*j5j3YvMgOfy4VoEMtuJGVA.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\"><img alt=\"\" class=\"bg mq mr c\" width=\"480\" height=\"480\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:720/1*j5j3YvMgOfy4VoEMtuJGVA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">Q matrix split across the Attention Heads (Image by Author)</figcaption></figure><p id=\"48bf\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">We are ready to compute the Attention Score.</p><h1 id=\"fcef\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Compute the Attention Score for each head</h1><p id=\"aba1\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">We now have the 3 matrices, Q, K, and V, split across the heads. These are used to compute the Attention Score.</p><p id=\"4da3\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">We will show the computations for a single head using just the last two dimensions (Sequence and Query size) and skip the first two dimensions (Batch and Head). Essentially, we can imagine that the computations we’re looking at are getting ‘repeated’ for each head and for each sample in the batch (although, obviously, they are happening as a single matrix operation, and not as a loop).</p><p id=\"c2f6\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The first step is to do a matrix multiplication between Q and K.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pn\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*BFteTbyGLg5T6x_VcOuHFg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BFteTbyGLg5T6x_VcOuHFg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BFteTbyGLg5T6x_VcOuHFg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BFteTbyGLg5T6x_VcOuHFg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BFteTbyGLg5T6x_VcOuHFg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BFteTbyGLg5T6x_VcOuHFg.png 1100w, https://miro.medium.com/v2/resize:fit:1000/format:webp/1*BFteTbyGLg5T6x_VcOuHFg.png 1000w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*BFteTbyGLg5T6x_VcOuHFg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*BFteTbyGLg5T6x_VcOuHFg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*BFteTbyGLg5T6x_VcOuHFg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*BFteTbyGLg5T6x_VcOuHFg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*BFteTbyGLg5T6x_VcOuHFg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*BFteTbyGLg5T6x_VcOuHFg.png 1100w, https://miro.medium.com/v2/resize:fit:1000/1*BFteTbyGLg5T6x_VcOuHFg.png 1000w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px\"><img alt=\"\" class=\"bg mq mr c\" width=\"500\" height=\"180\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:750/1*BFteTbyGLg5T6x_VcOuHFg.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"a0bf\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">A Mask value is now added to the result. In the Encoder Self-attention, the mask is used to mask out the Padding values so that they don’t participate in the Attention Score.</p><p id=\"12fa\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Different masks are applied in the Decoder Self-attention and in the Decoder Encoder-Attention which we’ll come to a little later in the flow.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md po\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*rhdN232CObcgMaXudTuwNw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*rhdN232CObcgMaXudTuwNw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*rhdN232CObcgMaXudTuwNw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*rhdN232CObcgMaXudTuwNw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*rhdN232CObcgMaXudTuwNw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*rhdN232CObcgMaXudTuwNw.png 1100w, https://miro.medium.com/v2/resize:fit:1110/format:webp/1*rhdN232CObcgMaXudTuwNw.png 1110w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 555px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*rhdN232CObcgMaXudTuwNw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*rhdN232CObcgMaXudTuwNw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*rhdN232CObcgMaXudTuwNw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*rhdN232CObcgMaXudTuwNw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*rhdN232CObcgMaXudTuwNw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*rhdN232CObcgMaXudTuwNw.png 1100w, https://miro.medium.com/v2/resize:fit:1110/1*rhdN232CObcgMaXudTuwNw.png 1110w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 555px\"><img alt=\"\" class=\"bg mq mr c\" width=\"555\" height=\"180\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:833/1*rhdN232CObcgMaXudTuwNw.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"9ee6\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The result is now scaled by dividing by the square root of the Query size, and then a Softmax is applied to it.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pn\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*nCwdrcb3bBO9fLu3vcClfg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*nCwdrcb3bBO9fLu3vcClfg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*nCwdrcb3bBO9fLu3vcClfg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*nCwdrcb3bBO9fLu3vcClfg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*nCwdrcb3bBO9fLu3vcClfg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*nCwdrcb3bBO9fLu3vcClfg.png 1100w, https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nCwdrcb3bBO9fLu3vcClfg.png 1000w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*nCwdrcb3bBO9fLu3vcClfg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*nCwdrcb3bBO9fLu3vcClfg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*nCwdrcb3bBO9fLu3vcClfg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*nCwdrcb3bBO9fLu3vcClfg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*nCwdrcb3bBO9fLu3vcClfg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*nCwdrcb3bBO9fLu3vcClfg.png 1100w, https://miro.medium.com/v2/resize:fit:1000/1*nCwdrcb3bBO9fLu3vcClfg.png 1000w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px\"><img alt=\"\" class=\"bg mq mr c\" width=\"500\" height=\"255\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:750/1*nCwdrcb3bBO9fLu3vcClfg.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"8988\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Another matrix multiplication is performed between the output of the Softmax and the V matrix.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*YQ8_I1EblTS-3WGbKUxUwA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*YQ8_I1EblTS-3WGbKUxUwA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*YQ8_I1EblTS-3WGbKUxUwA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*YQ8_I1EblTS-3WGbKUxUwA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*YQ8_I1EblTS-3WGbKUxUwA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YQ8_I1EblTS-3WGbKUxUwA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQ8_I1EblTS-3WGbKUxUwA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*YQ8_I1EblTS-3WGbKUxUwA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*YQ8_I1EblTS-3WGbKUxUwA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*YQ8_I1EblTS-3WGbKUxUwA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*YQ8_I1EblTS-3WGbKUxUwA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*YQ8_I1EblTS-3WGbKUxUwA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*YQ8_I1EblTS-3WGbKUxUwA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*YQ8_I1EblTS-3WGbKUxUwA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"248\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*YQ8_I1EblTS-3WGbKUxUwA.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"b230\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The complete Attention Score calculation in the Encoder Self-attention is as below:</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*NMneDGsvnXyOFqN6m8uSyA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NMneDGsvnXyOFqN6m8uSyA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NMneDGsvnXyOFqN6m8uSyA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NMneDGsvnXyOFqN6m8uSyA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NMneDGsvnXyOFqN6m8uSyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NMneDGsvnXyOFqN6m8uSyA.png 1100w, https://miro.medium.com/v2/resize:fit:960/format:webp/1*NMneDGsvnXyOFqN6m8uSyA.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*NMneDGsvnXyOFqN6m8uSyA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NMneDGsvnXyOFqN6m8uSyA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NMneDGsvnXyOFqN6m8uSyA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NMneDGsvnXyOFqN6m8uSyA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NMneDGsvnXyOFqN6m8uSyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NMneDGsvnXyOFqN6m8uSyA.png 1100w, https://miro.medium.com/v2/resize:fit:960/1*NMneDGsvnXyOFqN6m8uSyA.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\"><img alt=\"\" class=\"bg mq mr c\" width=\"480\" height=\"360\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:720/1*NMneDGsvnXyOFqN6m8uSyA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"eeaa\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Merge each Head’s Attention Scores together</h1><p id=\"0de7\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">We now have separate Attention Scores for each head, which need to be combined together into a single score. This Merge operation is essentially the reverse of the Split operation.</p><p id=\"6fcc\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">It is done by simply reshaping the result matrix to eliminate the Head dimension. The steps are:</p><ul class=\"\"><li id=\"d7fd\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Reshape the Attention Score matrix by swapping the Head and Sequence dimensions. In other words, the matrix shape goes from (Batch, Head, Sequence, Query size) to (Batch, Sequence, Head, Query size).</li><li id=\"91d9\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt ph ny nz bj\" data-selectable-paragraph=\"\">Collapse the Head dimension by reshaping to (Batch, Sequence, Head * Query size). This effectively concatenates the Attention Score vectors for each head into a single merged Attention Score.</li></ul><p id=\"abe7\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Since Embedding size =Head * Query size, the merged Score is (Batch, Sequence, Embedding size). In the picture below, we can see the complete process of merging for the example Score matrix.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*I-HmWc7Njuxf6HFxn6C8Hg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*I-HmWc7Njuxf6HFxn6C8Hg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*I-HmWc7Njuxf6HFxn6C8Hg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*I-HmWc7Njuxf6HFxn6C8Hg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*I-HmWc7Njuxf6HFxn6C8Hg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*I-HmWc7Njuxf6HFxn6C8Hg.png 1100w, https://miro.medium.com/v2/resize:fit:960/format:webp/1*I-HmWc7Njuxf6HFxn6C8Hg.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*I-HmWc7Njuxf6HFxn6C8Hg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*I-HmWc7Njuxf6HFxn6C8Hg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*I-HmWc7Njuxf6HFxn6C8Hg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*I-HmWc7Njuxf6HFxn6C8Hg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*I-HmWc7Njuxf6HFxn6C8Hg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*I-HmWc7Njuxf6HFxn6C8Hg.png 1100w, https://miro.medium.com/v2/resize:fit:960/1*I-HmWc7Njuxf6HFxn6C8Hg.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\"><img alt=\"\" class=\"bg mq mr c\" width=\"480\" height=\"450\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:720/1*I-HmWc7Njuxf6HFxn6C8Hg.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"4d39\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">End-to-end Multi-head Attention</h1><p id=\"281c\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">Putting it all together, this is the end-to-end flow of the Multi-head Attention.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*ArTXQZip_TwbU6gLshXOEw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ArTXQZip_TwbU6gLshXOEw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ArTXQZip_TwbU6gLshXOEw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ArTXQZip_TwbU6gLshXOEw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ArTXQZip_TwbU6gLshXOEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ArTXQZip_TwbU6gLshXOEw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ArTXQZip_TwbU6gLshXOEw.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"521\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*ArTXQZip_TwbU6gLshXOEw.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"0086\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Multi-head split captures richer interpretations</h1><p id=\"dfd4\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">An Embedding vector captures the meaning of a word. In the case of Multi-head Attention, as we have seen, the Embedding vectors for the input (and target) sequence gets logically split across multiple heads. What is the significance of this?</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*75EUBJLaqAMcDjgwWVh-_A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*75EUBJLaqAMcDjgwWVh-_A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*75EUBJLaqAMcDjgwWVh-_A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*75EUBJLaqAMcDjgwWVh-_A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*75EUBJLaqAMcDjgwWVh-_A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*75EUBJLaqAMcDjgwWVh-_A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*75EUBJLaqAMcDjgwWVh-_A.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*75EUBJLaqAMcDjgwWVh-_A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*75EUBJLaqAMcDjgwWVh-_A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*75EUBJLaqAMcDjgwWVh-_A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*75EUBJLaqAMcDjgwWVh-_A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*75EUBJLaqAMcDjgwWVh-_A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*75EUBJLaqAMcDjgwWVh-_A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*75EUBJLaqAMcDjgwWVh-_A.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"244\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*75EUBJLaqAMcDjgwWVh-_A.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"5f0a\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">This means that separate sections of the Embedding can learn different aspects of the meanings of each word, as it relates to other words in the sequence. This allows the Transformer to capture richer interpretations of the sequence.</p><p id=\"b649\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">This may not be a realistic example, but it might help to build intuition. For instance, one section might capture the ‘gender-ness’ (male, female, neuter) of a noun while another might capture the ‘cardinality’ (singular vs plural) of a noun. This might be important during translation because, in many languages, the verb that needs to be used depends on these factors.</p><h1 id=\"a470\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Decoder Self-Attention and Masking</h1><p id=\"9333\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The Decoder Self-Attention works just like the Encoder Self-Attention, except that it operates on each word of the target sequence.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*8Tkb3MUQl3iIIJKiLgRErQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*8Tkb3MUQl3iIIJKiLgRErQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*8Tkb3MUQl3iIIJKiLgRErQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*8Tkb3MUQl3iIIJKiLgRErQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*8Tkb3MUQl3iIIJKiLgRErQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*8Tkb3MUQl3iIIJKiLgRErQ.png 1100w, https://miro.medium.com/v2/resize:fit:960/format:webp/1*8Tkb3MUQl3iIIJKiLgRErQ.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*8Tkb3MUQl3iIIJKiLgRErQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*8Tkb3MUQl3iIIJKiLgRErQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*8Tkb3MUQl3iIIJKiLgRErQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*8Tkb3MUQl3iIIJKiLgRErQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*8Tkb3MUQl3iIIJKiLgRErQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*8Tkb3MUQl3iIIJKiLgRErQ.png 1100w, https://miro.medium.com/v2/resize:fit:960/1*8Tkb3MUQl3iIIJKiLgRErQ.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\"><img alt=\"\" class=\"bg mq mr c\" width=\"480\" height=\"400\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:720/1*8Tkb3MUQl3iIIJKiLgRErQ.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"001d\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Similarly, the Masking masks out the Padding words in the target sequence.</p><h1 id=\"2064\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Decoder Encoder-Decoder Attention and Masking</h1><p id=\"5adb\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The Encoder-Decoder Attention takes its input from two sources. Therefore, unlike the Encoder Self-Attention, which computes the interaction between each input word with other input words, and Decoder Self-Attention which computes the interaction between each target word with other target words, the Encoder-Decoder Attention computes the interaction between each target word with each input word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*El8DWgp2NAtF-08oCOVCIw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*El8DWgp2NAtF-08oCOVCIw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*El8DWgp2NAtF-08oCOVCIw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*El8DWgp2NAtF-08oCOVCIw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*El8DWgp2NAtF-08oCOVCIw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*El8DWgp2NAtF-08oCOVCIw.png 1100w, https://miro.medium.com/v2/resize:fit:960/format:webp/1*El8DWgp2NAtF-08oCOVCIw.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*El8DWgp2NAtF-08oCOVCIw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*El8DWgp2NAtF-08oCOVCIw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*El8DWgp2NAtF-08oCOVCIw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*El8DWgp2NAtF-08oCOVCIw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*El8DWgp2NAtF-08oCOVCIw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*El8DWgp2NAtF-08oCOVCIw.png 1100w, https://miro.medium.com/v2/resize:fit:960/1*El8DWgp2NAtF-08oCOVCIw.png 960w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 480px\"><img alt=\"\" class=\"bg mq mr c\" width=\"480\" height=\"420\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:720/1*El8DWgp2NAtF-08oCOVCIw.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"9921\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Therefore each cell in the resulting Attention Score corresponds to the interaction between one Q (ie. target sequence word) with all other K (ie. input sequence) words and all V (ie. input sequence) words.</p><p id=\"c0c7\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Similarly, the Masking masks out the later words in the target output, as was explained in detail in the <a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34\">second article</a> of the series.</p><h1 id=\"f31d\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Conclusion</h1><p id=\"49a1\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">Hopefully, this gives you a good sense of what the Attention modules in the Transformer do. When put together with the end-to-end flow of the Transformer as a whole that we went over in the second article, we have now covered the detailed operation of the entire Transformer architecture.</p><p id=\"16ad\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">We now understand exactly <em class=\"oa\">what</em> the Transformer does. But we haven’t fully answered the question of <em class=\"oa\">why</em> the Transformer’s Attention performs the calculations that it does. Why does it use the notions of Query, Key, and Value, and why does it perform the matrix multiplications that we just saw?</p><p id=\"8906\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">We have a vague intuitive idea that it ‘captures the relationship between each word with each other word’, but what exactly does that mean? How exactly does that give the Transformer’s Attention the capability to understand the nuances of each word in the sequence?</p><p id=\"ac01\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">That is an interesting question and is the subject of the final article of this series. Once we learn that, we will truly understand the elegance of the Transformer architecture.</p><p id=\"969d\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">And finally, if you liked this article, you might also enjoy my other series on Audio Deep Learning, Geolocation Machine Learning, and Image Caption architectures.</p></div></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
