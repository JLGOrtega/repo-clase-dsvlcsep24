{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMERS 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" class=\"bg mq mr c\" width=\"1000\" height=\"563\" loading=\"eager\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1500/0*dwfAOAIZEfya11Nl\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"ab ca\"><div class=\"ch bg dx dy dz ea\"><p id=\"0cf5\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">We’ve been hearing a lot about Transformers and with good reason. They have taken the world of NLP by storm in the last few years. The Transformer is an architecture that uses Attention to significantly improve the performance of deep learning NLP translation models. It was first introduced in the paper <a class=\"af mx\" href=\"https://arxiv.org/abs/1706.03762\" rel=\"noopener ugc nofollow\" target=\"_blank\">Attention is all you need</a> and was quickly established as the leading architecture for most text data applications.</p><p id=\"6ab5\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Since then, numerous projects including Google’s BERT and OpenAI’s GPT series have built on this foundation and published performance results that handily beat existing state-of-the-art benchmarks.</p><p id=\"3b16\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Over a series of articles, I’ll go over the basics of Transformers, its architecture, and how it works internally. We will cover the Transformer functionality in a top-down manner. In later articles, we will look under the covers to understand the operation of the system in detail. We will also do a deep dive into the workings of the multi-head attention, which is the heart of the Transformer.</p><p id=\"39bd\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Here’s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.</p><ol class=\"\"><li id=\"51a3\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Overview of functionality — this article</strong> <em class=\"oa\">(How Transformers are used, and why they are better than RNNs. Components of the architecture, and behavior during Training and Inference)</em></li><li id=\"03f4\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34\">How it works</a> <em class=\"oa\">(Internal operation end-to-end. How data flows and what computations are performed, including matrix representations)</em></li><li id=\"fee8\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\">Multi-head Attention</a> <em class=\"oa\">(Inner workings of the Attention module throughout the Transformer)</em></li><li id=\"b569\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3\">Why Attention Boosts Performance</a> <em class=\"oa\">(Not just what Attention does but why it works so well. How does Attention capture the relationships between words in a sentence)</em></li></ol><p id=\"e8a7\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">And if you’re interested in NLP applications in general, I have some other articles you might like.</p><ol class=\"\"><li id=\"1057\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24\">Beam Search</a> <em class=\"oa\">(Algorithm commonly used by Speech-to-Text and NLP applications to enhance predictions)</em></li><li id=\"07d4\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b\">Bleu Score</a> (<em class=\"oa\">Bleu Score and Word Error Rate are two essential metrics for NLP models</em>)</li></ol><h1 id=\"fe4e\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">What is a Transformer</h1><p id=\"5469\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer architecture excels at handling text data which is inherently sequential. They take a text sequence as input and produce another text sequence as output. eg. to translate an input English sentence to Spanish.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md ph\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 1100w, https://miro.medium.com/v2/resize:fit:656/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 656w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 328px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*XbPTo3Wi7q-HhwwhV4A9yA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XbPTo3Wi7q-HhwwhV4A9yA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XbPTo3Wi7q-HhwwhV4A9yA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XbPTo3Wi7q-HhwwhV4A9yA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XbPTo3Wi7q-HhwwhV4A9yA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XbPTo3Wi7q-HhwwhV4A9yA.png 1100w, https://miro.medium.com/v2/resize:fit:656/1*XbPTo3Wi7q-HhwwhV4A9yA.png 656w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 328px\"><img alt=\"\" class=\"bg mq mr c\" width=\"328\" height=\"344\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:492/1*XbPTo3Wi7q-HhwwhV4A9yA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"51b7\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">At its core, it contains a stack of Encoder layers and Decoder layers. To avoid confusion we will refer to the individual layer as an Encoder or a Decoder and will use Encoder stack or Decoder stack for a group of Encoder layers.</p><p id=\"8576\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Encoder stack and the Decoder stack each have their corresponding Embedding layers for their respective inputs. Finally, there is an Output layer to generate the final output.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pi\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1100w, https://miro.medium.com/v2/resize:fit:1040/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1040w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1100w, https://miro.medium.com/v2/resize:fit:1040/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1040w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px\"><img alt=\"\" class=\"bg mq mr c\" width=\"520\" height=\"610\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:780/1*XDtQ3C7XrtVuqTtrxr2UWQ.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"c64b\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">All the Encoders are identical to one another. Similarly, all the Decoders are identical.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 1100w, https://miro.medium.com/v2/resize:fit:1098/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 1098w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 549px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*F7JlVjpmv-XAEeE9IPyzHA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*F7JlVjpmv-XAEeE9IPyzHA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*F7JlVjpmv-XAEeE9IPyzHA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*F7JlVjpmv-XAEeE9IPyzHA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*F7JlVjpmv-XAEeE9IPyzHA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*F7JlVjpmv-XAEeE9IPyzHA.png 1100w, https://miro.medium.com/v2/resize:fit:1098/1*F7JlVjpmv-XAEeE9IPyzHA.png 1098w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 549px\"><img alt=\"\" class=\"bg mq mr c\" width=\"549\" height=\"315\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:824/1*F7JlVjpmv-XAEeE9IPyzHA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><ul class=\"\"><li id=\"5a76\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The Encoder contains the all-important Self-attention layer that computes the relationship between different words in the sequence, as well as a Feed-forward layer.</li><li id=\"613d\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The Decoder contains the Self-attention layer and the Feed-forward layer, as well as a second Encoder-Decoder attention layer.</li><li id=\"925b\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">Each Encoder and Decoder has its own set of weights.</li></ul><p id=\"af90\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Encoder is a reusable module that is the defining component of all Transformer architectures. In addition to the above two layers, it also has Residual skip connections around both layers along with two LayerNorm layers.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pl\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*THykpgtL058A9EpkstnUJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*THykpgtL058A9EpkstnUJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*THykpgtL058A9EpkstnUJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*THykpgtL058A9EpkstnUJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*THykpgtL058A9EpkstnUJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*THykpgtL058A9EpkstnUJQ.png 1100w, https://miro.medium.com/v2/resize:fit:490/format:webp/1*THykpgtL058A9EpkstnUJQ.png 490w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 245px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*THykpgtL058A9EpkstnUJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*THykpgtL058A9EpkstnUJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*THykpgtL058A9EpkstnUJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*THykpgtL058A9EpkstnUJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*THykpgtL058A9EpkstnUJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*THykpgtL058A9EpkstnUJQ.png 1100w, https://miro.medium.com/v2/resize:fit:490/1*THykpgtL058A9EpkstnUJQ.png 490w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 245px\"><img alt=\"\" class=\"bg mq mr c\" width=\"245\" height=\"409\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:368/1*THykpgtL058A9EpkstnUJQ.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"306a\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">There are many variations of the Transformer architecture. Some Transformer architectures have no Decoder at all and rely only on the Encoder.</p><h1 id=\"27c9\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">What does Attention Do?</h1><p id=\"9f2d\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The key to the Transformer’s ground-breaking performance is its use of Attention.</p><p id=\"2e88\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><mark class=\"aed aee ao\">While processing a word, Attention enables the model to focus on other words in the input that are closely related to that word.</mark></p><p id=\"7d05\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">eg. ‘Ball’ is closely related to ‘blue’ and ‘holding’. On the other hand, ‘blue’ is not related to ‘boy’.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 1100w, https://miro.medium.com/v2/resize:fit:634/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 634w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 317px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*1ouB-xrMxPgqu721_zzsbA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*1ouB-xrMxPgqu721_zzsbA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*1ouB-xrMxPgqu721_zzsbA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*1ouB-xrMxPgqu721_zzsbA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*1ouB-xrMxPgqu721_zzsbA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*1ouB-xrMxPgqu721_zzsbA.png 1100w, https://miro.medium.com/v2/resize:fit:634/1*1ouB-xrMxPgqu721_zzsbA.png 634w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 317px\"><img alt=\"\" class=\"bg mq mr c\" width=\"317\" height=\"91\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:476/1*1ouB-xrMxPgqu721_zzsbA.png\"></picture></div></figure><p id=\"1251\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer architecture uses self-attention by relating every word in the input sequence to every other word.</p><p id=\"47ee\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">eg. Consider two sentences:</p><ul class=\"\"><li id=\"88b2\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The <em class=\"oa\">cat</em> drank the milk because <strong class=\"na fh\">it</strong> was hungry.</li><li id=\"3b9d\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The cat drank the <em class=\"oa\">milk</em> because <strong class=\"na fh\">it</strong> was sweet.</li></ul><p id=\"a816\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In the first sentence, the word ‘it’ refers to ‘cat’, while in the second it refers to ‘milk. When the model processes the word ‘it’, self-attention gives the model more information about its meaning so that it can associate ‘it’ with the correct word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pn\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*pT0ZIWeoilLkz3e_1fVeYQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pT0ZIWeoilLkz3e_1fVeYQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pT0ZIWeoilLkz3e_1fVeYQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pT0ZIWeoilLkz3e_1fVeYQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pT0ZIWeoilLkz3e_1fVeYQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"389\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*pT0ZIWeoilLkz3e_1fVeYQ.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">Dark colors represent higher attention (Image by Author)</figcaption></figure><p id=\"e8d9\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">To enable it to handle more nuances about the intent and semantics of the sentence, Transformers include multiple attention scores for each word.</p><p id=\"1b7b\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">eg. While processing the word ‘it’, the first score highlights ‘cat’, while the second score highlights ‘hungry’. So when it decodes the word ‘it’, by translating it into a different language, for instance, it will incorporate some aspect of both ‘cat’ and ‘hungry’ into the translated word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md po\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 1100w, https://miro.medium.com/v2/resize:fit:984/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 984w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 492px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*s2hugsMP28aB2tJoYCq8uA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*s2hugsMP28aB2tJoYCq8uA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*s2hugsMP28aB2tJoYCq8uA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*s2hugsMP28aB2tJoYCq8uA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*s2hugsMP28aB2tJoYCq8uA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*s2hugsMP28aB2tJoYCq8uA.png 1100w, https://miro.medium.com/v2/resize:fit:984/1*s2hugsMP28aB2tJoYCq8uA.png 984w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 492px\"><img alt=\"\" class=\"bg mq mr c\" width=\"492\" height=\"557\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:738/1*s2hugsMP28aB2tJoYCq8uA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"a5ce\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Training the Transformer</h1><p id=\"3982\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer works slightly differently during Training and while doing Inference.</p><p id=\"bdd4\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Let’s first look at the flow of data during Training. Training data consists of two parts:</p><ul class=\"\"><li id=\"8019\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The source or input sequence (eg. “You are welcome” in English, for a translation problem)</li><li id=\"6dc0\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The destination or target sequence (eg. “De nada” in Spanish)</li></ul><p id=\"73a9\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer’s goal is to learn how to output the target sequence, by using both the input and target sequence.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pp\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*0g4qdq7Rt6QvDalFFAkL5g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*0g4qdq7Rt6QvDalFFAkL5g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*0g4qdq7Rt6QvDalFFAkL5g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*0g4qdq7Rt6QvDalFFAkL5g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*0g4qdq7Rt6QvDalFFAkL5g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*0g4qdq7Rt6QvDalFFAkL5g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*0g4qdq7Rt6QvDalFFAkL5g.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"521\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*0g4qdq7Rt6QvDalFFAkL5g.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"1443\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer processes the data like this:</p><ol class=\"\"><li id=\"e762\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The input sequence is converted into Embeddings (with Position Encoding) and fed to the Encoder.</li><li id=\"808b\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Encoders processes this and produces an encoded representation of the input sequence.</li><li id=\"fe17\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The target sequence is prepended with a start-of-sentence token, converted into Embeddings (with Position Encoding), and fed to the Decoder.</li><li id=\"5b25\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Decoders processes this along with the Encoder stack’s encoded representation to produce an encoded representation of the target sequence.</li><li id=\"bbd2\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The Output layer converts it into word probabilities and the final output sequence.</li><li id=\"51ef\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The Transformer’s Loss function compares this output sequence with the target sequence from the training data. This loss is used to generate gradients to train the Transformer during back-propagation.</li></ol><h1 id=\"96e2\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Inference</h1><p id=\"8b0f\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">During Inference, we have only the input sequence and don’t have the target sequence to pass as input to the Decoder. The goal of the Transformer is to produce the target sequence from the input sequence alone.</p><p id=\"dae1\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">So, like in a Seq2Seq model, we generate the output in a loop and feed the output sequence from the previous timestep to the Decoder in the next timestep until we come across an end-of-sentence token.</p><p id=\"e6d9\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The difference from the Seq2Seq model is that, at each timestep, we re-feed the entire output sequence generated thus far, rather than just the last word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pp\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*-uvybwr8xULd3ug9ZwcSaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-uvybwr8xULd3ug9ZwcSaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-uvybwr8xULd3ug9ZwcSaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-uvybwr8xULd3ug9ZwcSaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-uvybwr8xULd3ug9ZwcSaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-uvybwr8xULd3ug9ZwcSaQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-uvybwr8xULd3ug9ZwcSaQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"521\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*-uvybwr8xULd3ug9ZwcSaQ.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">Inference flow, after first timestep (Image by Author)</figcaption></figure><p id=\"32ec\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The flow of data during Inference is:</p><ol class=\"\"><li id=\"72ee\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The input sequence is converted into Embeddings (with Position Encoding) and fed to the Encoder.</li><li id=\"ddc3\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Encoders processes this and produces an encoded representation of the input sequence.</li><li id=\"a545\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">Instead of the target sequence, we use an empty sequence with only a start-of-sentence token. This is converted into Embeddings (with Position Encoding) and fed to the Decoder.</li><li id=\"0834\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Decoders processes this along with the Encoder stack’s encoded representation to produce an encoded representation of the target sequence.</li><li id=\"bcb9\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The Output layer converts it into word probabilities and produces an output sequence.</li><li id=\"d5f1\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">We take the last word of the output sequence as the predicted word. That word is now filled into the second position of our Decoder input sequence, which now contains a start-of-sentence token and the first word.</li><li id=\"a0cd\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">Go back to step #3. As before, feed the new Decoder sequence into the model. Then take the second word of the output and append it to the Decoder sequence. Repeat this until it predicts an end-of-sentence token. Note that since the Encoder sequence does not change for each iteration, we do not have to repeat steps #1 and #2 each time (<em class=\"oa\">Thanks to Michal Kučírka for pointing this out</em>).</li></ol><h1 id=\"63d2\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Teacher Forcing</h1><p id=\"7f8f\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The approach of feeding the target sequence to the Decoder during training is known as Teacher Forcing. Why do we do this and what does that term mean?</p><p id=\"69eb\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">During training, we could have used the same approach that is used during inference. In other words, run the Transformer in a loop, take the last word from the output sequence, append it to the Decoder input and feed it to the Decoder for the next iteration. Finally, when the end-of-sentence token is predicted, the Loss function would compare the generated output sequence to the target sequence in order to train the network.</p><p id=\"0cbe\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Not only would this looping cause training to take much longer, but it also makes it harder to train the model. The model would have to predict the second word based on a potentially erroneous first predicted word, and so on.</p><p id=\"05a2\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Instead, by feeding the target sequence to the Decoder, we are giving it a hint, so to speak, just like a Teacher would. Even though it predicted an erroneous first word, it can instead use the correct first word to predict the second word so that those errors don’t keep compounding.</p><p id=\"2fdd\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In addition, the Transformer is able to output all the words in parallel without looping, which greatly speeds up training.</p><h1 id=\"0e11\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">What are Transformers used for?</h1><p id=\"68a6\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">Transformers are very versatile and are used for most NLP tasks such as language models and text classification. They are frequently used in sequence-to-sequence models for applications such as Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, and Speech Recognition.</p><p id=\"a5f0\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">There are different flavors of the Transformer architecture for different problems. The basic Encoder Layer is used as a common building block for these architectures, with different application-specific ‘heads’ depending on the problem being solved.</p><h2 id=\"37d8\" class=\"pq oh ev be oi pr ps pt ol pu pv pw oo nh px py pz nl qa qb qc np qd qe qf fc bj\" data-selectable-paragraph=\"\">Transformer Classification architecture</h2><p id=\"32b4\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">A Sentiment Analysis application, for instance, would take a text document as input. A Classification head takes the Transformer’s output and generates predictions of the class labels such as a positive or negative sentiment.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md qg\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*tkqBjeTRZMRfOLiLqYV0TA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*tkqBjeTRZMRfOLiLqYV0TA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*tkqBjeTRZMRfOLiLqYV0TA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*tkqBjeTRZMRfOLiLqYV0TA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*tkqBjeTRZMRfOLiLqYV0TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*tkqBjeTRZMRfOLiLqYV0TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*tkqBjeTRZMRfOLiLqYV0TA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"185\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*tkqBjeTRZMRfOLiLqYV0TA.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h2 id=\"7ba8\" class=\"pq oh ev be oi pr ps pt ol pu pv pw oo nh px py pz nl qa qb qc np qd qe qf fc bj\" data-selectable-paragraph=\"\">Transformer Language Model architecture</h2><p id=\"26a7\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">A Language Model architecture would take the initial part of an input sequence such as a text sentence as input, and generate new text by predicting sentences that would follow. A Language Model head takes the Transformer’s output and generates a probability for every word in the vocabulary. The highest probability word becomes the predicted output for the next word in the sentence.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md qh\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*9HgXzK95-QTOQi5eyZ-S_Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*9HgXzK95-QTOQi5eyZ-S_Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*9HgXzK95-QTOQi5eyZ-S_Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*9HgXzK95-QTOQi5eyZ-S_Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*9HgXzK95-QTOQi5eyZ-S_Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"184\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*9HgXzK95-QTOQi5eyZ-S_Q.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"c10b\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">How are they better than RNNs?</h1><p id=\"9697\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">RNNs and their cousins, LSTMs and GRUs, were the de facto architecture for all NLP applications until Transformers came along and dethroned them.</p><p id=\"9f77\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">RNN-based sequence-to-sequence models performed well, and when the Attention mechanism was first introduced, it was used to enhance their performance.</p><p id=\"5ce8\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">However, they had two limitations:</p><ul class=\"\"><li id=\"5b30\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">It was challenging to deal with long-range dependencies between words that were spread far apart in a long sentence.</li><li id=\"283e\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">They process the input sequence sequentially one word at a time, which means that it cannot do the computation for time-step <em class=\"oa\">t</em> until it has completed the computation for time-step <em class=\"oa\">t — 1</em>. This slows down training and inference.</li></ul><p id=\"9b0a\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">As an aside, with CNNs, all of the outputs can be computed in parallel, which makes convolutions much faster. However, they also have limitations in dealing with long-range dependencies:</p><ul class=\"\"><li id=\"813f\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">In a convolutional layer, only parts of the image (or words if applied to text data) that are close enough to fit within the kernel size can interact with each other. For items that are further apart, you need a much deeper network with many layers.</li></ul><p id=\"0d58\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer architecture addresses both of these limitations. It got rid of RNNs altogether and relied exclusively on the benefits of Attention.</p><ul class=\"\"><li id=\"6a4d\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">They process all the words in the sequence in parallel, thus greatly speeding up computation.</li></ul><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md qi\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*Iygs9mQi4GbIJuc6fwBRKg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Iygs9mQi4GbIJuc6fwBRKg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Iygs9mQi4GbIJuc6fwBRKg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Iygs9mQi4GbIJuc6fwBRKg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Iygs9mQi4GbIJuc6fwBRKg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Iygs9mQi4GbIJuc6fwBRKg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Iygs9mQi4GbIJuc6fwBRKg.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"263\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*Iygs9mQi4GbIJuc6fwBRKg.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><ul class=\"\"><li id=\"2508\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The distance between words in the input sequence does not matter. It is equally good at computing dependencies between adjacent words and words that are far apart.</li></ul><p id=\"f9e5\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Now that we have a high-level idea of what a Transformer is, we can go deeper into its internal functionality in the next article to understand the details of how it works.</p></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_next_token(probabilities, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Selecciona un token basado en las probabilidades ajustadas por la temperatura.\n",
    "    \n",
    "    :param probabilities: lista o array de probabilidades (deben sumar 1.0)\n",
    "    :param temperature: valor de temperatura ( > 0 ). \n",
    "                        - Un valor bajo (ej: 0.2) hace las predicciones más deterministas.\n",
    "                        - Un valor alto (ej: 1.0 o 1.5) añade más diversidad.\n",
    "    :return: índice del token seleccionado.\n",
    "    \"\"\"\n",
    "    # Convertimos a array numpy por conveniencia\n",
    "    probs = np.array(probabilities, dtype=float)\n",
    "    \n",
    "    # Evitar el caso de temperatura 0, que no tiene sentido, se pone un mínimo\n",
    "    if temperature <= 1e-8:\n",
    "        temperature = 1e-8\n",
    "\n",
    "    # Ajustar las probabilidades con la temperatura\n",
    "    # probs^(1/temperatura) tiende a \"suavizar\" o \"afilar\" la distribución\n",
    "    adjusted_probs = np.power(probs, 1.0 / temperature)\n",
    "    \n",
    "    # Normalizamos para que la suma sea 1\n",
    "    adjusted_probs /= np.sum(adjusted_probs)\n",
    "    \n",
    "    # Realizamos una selección aleatoria ponderada por las nuevas probabilidades\n",
    "    token_index = np.random.choice(len(adjusted_probs), p=adjusted_probs)\n",
    "    return token_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_probs = [0.5, 0.3, 0.1, 0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79370053, 0.66943295, 0.46415888, 0.36840315, 0.36840315])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(np.array(original_probs), 1 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo de uso:\n",
    "# Supongamos que tenemos 5 tokens con las siguientes probabilidades:\n",
    "original_probs = [0.5, 0.3, 0.1, 0.05, 0.05]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selección con temperatura baja (más determinista)\n",
    "token_seleccionado_baja = sample_next_token(original_probs, temperature=0.2)\n",
    "print(\"Token seleccionado con temp 0.2:\", token_seleccionado_baja)\n",
    "\n",
    "# Selección con temperatura intermedia\n",
    "token_seleccionado_media = sample_next_token(original_probs, temperature=1.0)\n",
    "print(\"Token seleccionado con temp 1.0:\", token_seleccionado_media)\n",
    "\n",
    "# Selección con temperatura alta (más aleatoria)\n",
    "token_seleccionado_alta = sample_next_token(original_probs, temperature=2.0)\n",
    "print(\"Token seleccionado con temp 2.0:\", token_seleccionado_alta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"COHERE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere \n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = co.generate(model=\"command\", prompt=\"Whats the name of my cousin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" It's difficult to provide an answer here without any additional information. Could you provide some context or additional details to assist me in providing a more accurate response? \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta.generations[0].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASISTENGTE VIRTUAL (press q to exit)\n",
      "Hello!\n",
      " Hi! I'm a chatbot, and I can help you with any task you require. Such as writing, brainstorming, and answering questions. How can I help you today? \n",
      "are you handsome?\n",
      " I apologize if my inability to provide you with a subjective opinion on beauty frustrates you in any way, as I am designed to be helpful and harmless. \n",
      "\n",
      "Would you like me to help you with any other task you had in mind? If you would like to continue the conversation, I am here to help you with anything else you may need. \n",
      "are you there?\n",
      " Yes, I am here! I am ready to assist you with any questions or tasks you have. Feel free to ask me whatever you like, and I'll do my best to respond appropriately and safely. \n",
      "\n",
      "I look forward to helping you! \n"
     ]
    }
   ],
   "source": [
    "import cohere \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "hello =\"ASISTENGTE VIRTUAL (press q to exit)\"\n",
    "print(hello)\n",
    "history = []\n",
    "while True:\n",
    "    msg = input(\"User: \")\n",
    "    if msg == \"q\":\n",
    "        break\n",
    "    prompt = \"\\n\".join(history)+\"\\n\" \n",
    "    prompt += msg\n",
    "    respuesta = co.generate(model=\"command\", prompt=prompt)\n",
    "    print(msg)\n",
    "    print(respuesta.generations[0].text)\n",
    "    history.append(msg)\n",
    "    history.append(respuesta.generations[0].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import cohere\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Cohere API\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "# Function to chunk text manually (by character length)\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to get embeddings using Cohere's API\n",
    "def get_embedding(text):\n",
    "    input_type=\"search_query\"\n",
    "    response = co.embed(texts=[text],\n",
    "                         model=\"embed-english-v3.0\",  \n",
    "                         input_type=input_type,\n",
    "                         embedding_types=['float'])\n",
    "    return response.embeddings.float[0]\n",
    "\n",
    "# Function to generate response using Cohere's generation API\n",
    "def generate_response(context, query):\n",
    "    response = co.generate(\n",
    "        model=\"command-r-plus\",\n",
    "        prompt=f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
    "        max_tokens=512,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.generations[0].text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texto_muestra.txt\", \"r\") as f:\n",
    "    plain_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ensayo sobre la fenomenología y el papel ecológico de los insectos fluorescentes: una aproximación imaginaria**\n",
      "\n",
      "*Introducción*  \n",
      "La naturaleza está repleta de criaturas extraordinarias cuyas características aún sorprenden a la comunidad científica. Entre ellas, los insectos fluorescentes constituyen un enigma particularmente fascinante. Si bien el concepto de “insectos fluorescentes” no figura con amplitud en la literatura científica convencional, este ensayo imaginario busca explorar, a través de una mirada especulativa y creativa, las posibles cualidades biológicas, ecológicas y culturales que podrían caracterizar a estos hipotéticos seres. Este texto, por lo tanto, no debe ser entendido como un informe científico fidedigno, sino más bien como una construcción literaria e intelectual que toma prestados ciertos principios biológicos para crear un mundo hipotético en el que pequeños artrópodos producen luz fluorescente bajo condiciones específicas.\n",
      "\n",
      "*Contexto y motivaciones teóricas*  \n",
      "La fluorescencia, entendida como la emisión de luz de baja intensidad tras la absorción de radiación de alta energía, se observa en múltiples organismos, incluyendo algunos tipos de peces, corales y hasta escorpiones. Sin embargo, la idea de insectos que brillan con luz fluorescente bajo la tenue iluminación de la luna o los focos de nuestros laboratorios es un concepto menos abordado. La motivación de este ensayo radica en suponer una diversidad de especies de insectos capaces de emitir luz mediante un mecanismo no estrictamente bioluminiscente (como el de las luciérnagas) sino auténticamente fluorescente: sus cuerpos absorberían luz ultravioleta proveniente del sol o del ambiente y la reemitirían en longitudes de onda visibles al ojo humano. Estos seres, hipotéticamente, formarían parte de nichos ecológicos específicos, aportando una dinámica lumínica singular a los hábitats que ocupan.\n",
      "\n",
      "*La evolución hipotética de la fluorescencia en insectos*  \n",
      "En la escala temporal evolutiva, la fluorescencia en insectos podría haber surgido como una respuesta adaptativa a una combinación de factores ambientales. Imagine un bosque tropical con un denso dosel que filtra la mayor parte de la luz solar, dejando solamente un leve resplandor ultravioleta. En este ambiente, algunos insectos habrían desarrollado pigmentos especializados en su exoesqueleto capaces de absorber la radiación ultravioleta residual y reemitirla en forma de luz visible, creando un juego cromático de luces verdes, azules o incluso rosadas. Esta fluorescencia podría servir a múltiples propósitos:  \n",
      "1. **Comunicación intraespecífica:** La emisión de ciertos patrones fluorescentes podría facilitar el reconocimiento entre machos y hembras durante el cortejo, reemplazando las señales feromonales convencionales o complementándolas.  \n",
      "2. **Señalización de advertencia:** Si bien el veneno, los sabores amargos o la presencia de estructuras defensivas son estrategias conocidas, la fluorescencia podría ser una advertencia visual a depredadores nocturnos que han aprendido a asociar el brillo con un sabor desagradable o toxinas perjudiciales.  \n",
      "3. **Camuflaje cromático inverso:** En entornos ricos en hongos fluorescentes o líquenes bioluminiscentes, la fluorescencia podría servir para mimetizarse con el sustrato, confundiendo la percepción visual de los depredadores y convirtiéndose en una forma inesperada de camuflaje.\n",
      "\n",
      "*Bases fisiológicas imaginarias*  \n",
      "En un insecto fluorescente hipotético, podrían encontrarse en su cutícula sustancias químicas como las fluoroquinas o polipirroles modificados, capaces de absorber la luz ultravioleta y, tras un proceso fotoquímico interno, emitir luz visible. Los órganos responsables de esta propiedad serían minúsculas placas incrustadas entre las capas de quitina, con pigmentos estructurados en nanoesferas o nanotúneles que maximizarían la eficiencia de la absorción y la posterior emisión. Además, los insectos podrían regular la intensidad del brillo gracias a microválvulas pigmentarias que controlan la cantidad de luz reemitida, dependiendo del contexto: un macho en cortejo, por ejemplo, podría amplificar su brillo, mientras que un insecto que desea pasar inadvertido podría reducirlo, difuminando su propio resplandor.\n",
      "\n",
      "*Comportamiento social y dinámicas de grupo*  \n",
      "En una colonia hipotética de insectos fluorescentes, las interacciones sociales podrían estar profundamente influidas por los patrones lumínicos. Así como las abejas se comunican mediante danzas y vibraciones, estos insectos podrían entablar diálogos visuales mediante destellos sincronizados. Por ejemplo, un grupo de insectos reunidos en torno a una fuente de alimento podría emitir pulsaciones lumínicas rítmicas para advertir a los miembros de la colonia. En la construcción de nidos o galerías subterráneas, la fluorescencia podría servir como sistema de señalización interna, guiando a los obreros por túneles oscuros. Imaginemos una jerarquía lumínica, donde los ejemplares más dominantes brillen con mayor intensidad, o presenten patrones fluorescentes más complejos, reflejando su posición social en la colonia.\n",
      "\n",
      "*Rol ecológico en un hábitat ficticio*  \n",
      "Pensemos ahora en un valle remoto rodeado de montañas que filtran la luz, creando un ambiente permanentemente crepuscular, rico en musgos y líquenes fluorescentes. En este ecosistema imaginario, los insectos fluorescentes formarían parte integral de la red trófica. Serían polinizadores especializados de flores con pétalos que absorben la radiación ultravioleta del ambiente. Estas flores, para atraer a sus polinizadores, podrían haber desarrollado patrones florales que sólo resultan visibles bajo la fluorescencia de los insectos. Dichos patrones podrían contener guías luminosas que indican la ubicación del néctar, generando una relación simbiótica: las flores facilitan el acceso al alimento, y los insectos, al posarse y liberar polen sobre sus cuerpos, dispersan el material genético a otros especímenes vegetales, garantizando la reproducción continua del ecosistema. A su vez, algunos depredadores nocturnos, como pequeños murciélagos o anfibios biológicamente adaptados para detectar la luz fluorescente, podrían basar su estrategia de caza en reconocer los destellos lumínicos de estos insectos, equilibrando así la estructura poblacional del entorno.\n",
      "\n",
      "*Interacciones con el ser humano*  \n",
      "En un plano imaginario en el que la humanidad convive con tales insectos, surgen infinidad de posibilidades culturales. Artistas inspirados en la fluorescencia podrían crear pinturas que brillan bajo lámparas ultravioletas, intentando imitar la complejidad cromática que estos insectos presentan de manera natural. La joyería artesanal podría incorporar fragmentos de exoesqueletos caídos, utilizándolos como gemas orgánicas luminiscentes. La medicina tradicional, por su parte, podría atribuir propiedades curativas a la luz emitida por estos seres, empleándolos en rituales nocturnos o creando ungüentos con extractos fluorescentes que, en una percepción mágica, ayudarían a equilibrar las energías del paciente.\n",
      "\n",
      "Además, la ciencia humana hipotéticamente interesada en tales insectos podría descubrir en sus pigmentos nuevas rutas para el desarrollo de tintas de seguridad, sistemas de marcaje lumínico en microcirugía, o sensores biológicos capaces de detectar contaminantes ambientales. La tecnología bioinspirada, de hecho, tendría un campo fértil en estas criaturas lumínicas: imagine pequeños drones de exploración recubiertos con pigmentos fluorescentes, capaces de interactuar con el entorno visualmente, “comunicándose” con sensores basados en la misma dinámica óptica.\n",
      "\n",
      "*Ética, conservación y dilemas imaginarios*  \n",
      "Dado que este ensayo se mueve en un terreno especulativo, es legítimo abordar también la dimensión ética. La sobreexplotación de insectos fluorescentes podría surgir si sus componentes químicos fueran cotizados en el mercado ilegal. Su captura indiscriminada pondría en riesgo a poblaciones enteras, provocando un colapso ecológico en el mundo ficticio que hemos delineado. El turismo lumínico, en el que aventureros visitan cuevas o bosques en busca de estos “faroles vivientes”, podría alterar significativamente sus patrones de conducta, afectar su reproducción y, en última instancia, poner en jaque su supervivencia.\n",
      "\n",
      "Como contrapartida, surgirían movimientos de conservación y santuarios naturales dedicados a proteger estas especies únicas. Instituciones dedicadas a la educación ambiental podrían aprovechar la fascinación que generan estos insectos para enseñar a las personas sobre la complejidad de las redes tróficas, las relaciones simbióticas y las consecuencias de la alteración humana en el medioambiente. Este rol pedagógico tendría un impacto notable en la conciencia colectiva, generando un mayor respeto hacia la biodiversidad luminosa.\n",
      "\n",
      "*Conclusiones*  \n",
      "La creación imaginaria de insectos fluorescentes nos permite reflexionar sobre la riqueza potencial de las estrategias biológicas, la complejidad de los ecosistemas y las posibles influencias recíprocas entre el ser humano y el mundo natural. Aunque estos insectos no existen tal como han sido descritos aquí, su hipotética presencia nos conduce a preguntarnos hasta qué punto desconocemos las variantes asombrosas que la evolución podría producir bajo las condiciones ambientales adecuadas. El ejercicio intelectual que acabamos de explorar, de este modo, sirve no sólo como entretenimiento creativo, sino también como punto de partida para valorar la diversidad real de la vida terrestre, comprendiendo que el universo biológico es tan vasto e impredecible que la fluorescencia de un insecto podría ser tan sólo una pincelada más en el lienzo inabarcable de la naturaleza.\n"
     ]
    }
   ],
   "source": [
    "print(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 10\n"
     ]
    }
   ],
   "source": [
    "# 2. Chunk the text\n",
    "chunks = chunk_text(plain_text, chunk_size=1000) ## CUIDADO CON EL NUMERO DE LLAMADAS\n",
    "print(f\"Text split into {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ensayo sobre la fenomenología y el papel ecológico de los insectos fluorescentes: una aproximación imaginaria**\n",
      "\n",
      "*Introducción*  \n",
      "La naturaleza está repleta de criaturas extraordinarias cuyas características aún sorprenden a la comunidad científica. Entre ellas, los insectos fluorescentes constituyen un enigma particularmente fascinante. Si bien el concepto de “insectos fluorescentes” no figura con amplitud en la literatura científica convencional, este ensayo imaginario busca explorar, a través de una mirada especulativa y creativa, las posibles cualidades biológicas, ecológicas y culturales que podrían caracterizar a estos hipotéticos seres. Este texto, por lo tanto, no debe ser entendido como un informe científico fidedigno, sino más bien como una construcción literaria e intelectual que toma prestados ciertos principios biológicos para crear un mundo hipotético en el que pequeños artrópodos producen luz fluorescente bajo condiciones específicas.\n",
      "\n",
      "*Contexto y motivaciones teóricas\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x0000021CCF2BF0C0> >"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3. Embed each chunk and create a FAISS index\n",
    "dimension = 1024  # Cohere embed-english-v3.0 dimension\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x0000021CCF2BF0C0> >"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convert embeddings to numpy array and add to FAISS\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "index.add(embeddings)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03771973,  0.01338196,  0.00738907, ..., -0.00264549,\n",
       "         0.01875305,  0.04263306]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Query and retrieve relevant chunks\n",
    "query = \"Hablame de la señalizacion de advertencia\"\n",
    "query_embedding = np.array(get_embedding(query)).astype('float32').reshape(1, -1)\n",
    "query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform search\n",
    "k = 1  # Top-1 most relevant chunk\n",
    "distances, indices = index.search(query_embedding, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 9, 8, 3, 0]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'la fluorescencia en insectos podría haber surgido como una respuesta adaptativa a una combinación de factores ambientales. Imagine un bosque tropical con un denso dosel que filtra la mayor parte de la luz solar, dejando solamente un leve resplandor ultravioleta. En este ambiente, algunos insectos habrían desarrollado pigmentos especializados en su exoesqueleto capaces de absorber la radiación ultravioleta residual y reemitirla en forma de luz visible, creando un juego cromático de luces verdes, azules o incluso rosadas. Esta fluorescencia podría servir a múltiples propósitos:  \\n1. **Comunicación intraespecífica:** La emisión de ciertos patrones fluorescentes podría facilitar el reconocimiento entre machos y hembras durante el cortejo, reemplazando las señales feromonales convencionales o complementándolas.  \\n2. **Señalización de advertencia:** Si bien el veneno, los sabores amargos o la presencia de estructuras defensivas son estrategias conocidas, la fluorescencia podría ser una adver'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 9, 8, 3, 0, 7, 4, 6, 1, 5]], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Hablame de la señalizacion de advertencia\n",
      "Retrieved chunk: la fluorescencia en insectos podría haber surgido como una respuesta adaptativa a una combinación de factores ambientales. Imagine un bosque tropical con un denso dosel que filtra la mayor parte de la luz solar, dejando solamente un leve resplandor ultravioleta. En este ambiente, algunos insectos habrían desarrollado pigmentos especializados en su exoesqueleto capaces de absorber la radiación ultravioleta residual y reemitirla en forma de luz visible, creando un juego cromático de luces verdes, azules o incluso rosadas. Esta fluorescencia podría servir a múltiples propósitos:  \n",
      "1. **Comunicación intraespecífica:** La emisión de ciertos patrones fluorescentes podría facilitar el reconocimiento entre machos y hembras durante el cortejo, reemplazando las señales feromonales convencionales o complementándolas.  \n",
      "2. **Señalización de advertencia:** Si bien el veneno, los sabores amargos o la presencia de estructuras defensivas son estrategias conocidas, la fluorescencia podría ser una adver - - - ológicas, la complejidad de los ecosistemas y las posibles influencias recíprocas entre el ser humano y el mundo natural. Aunque estos insectos no existen tal como han sido descritos aquí, su hipotética presencia nos conduce a preguntarnos hasta qué punto desconocemos las variantes asombrosas que la evolución podría producir bajo las condiciones ambientales adecuadas. El ejercicio intelectual que acabamos de explorar, de este modo, sirve no sólo como entretenimiento creativo, sino también como punto de partida para valorar la diversidad real de la vida terrestre, comprendiendo que el universo biológico es tan vasto e impredecible que la fluorescencia de un insecto podría ser tan sólo una pincelada más en el lienzo inabarcable de la naturaleza.\n",
      "\n",
      "Generated Answer: La fluorescencia en insectos también podría funcionar como una señal de advertencia. De la misma manera que algunas especies de ranas o serpientes tienen colores brillantes para indicar su toxicidad, los insectos fluorescentes podrían estar indicando a los depredadores que son venenosos o que poseen defensas potentes. Esta estrategia visual podría ser particularmente efectiva en un ambiente con poca luz, donde la fluorescencia llamativa sería más evidente y disuasiva.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the most relevant chunk\n",
    "retrieved_chunk = chunks[indices[0][0]] +\" - - - \" +chunks[indices[0][1]]\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"Retrieved chunk:\", retrieved_chunk)\n",
    "\n",
    "# 5. Generate response using the retrieved chunk as context\n",
    "generated_answer = generate_response(retrieved_chunk, query)\n",
    "print(\"\\nGenerated Answer:\", generated_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: \n",
      "Retrieved chunk: rantizando la reproducción continua del ecosistema. A su vez, algunos depredadores nocturnos, como pequeños murciélagos o anfibios biológicamente adaptados para detectar la luz fluorescente, podrían basar su estrategia de caza en reconocer los destellos lumínicos de estos insectos, equilibrando así la estructura poblacional del entorno.\n",
      "\n",
      "*Interacciones con el ser humano*  \n",
      "En un plano imaginario en el que la humanidad convive con tales insectos, surgen infinidad de posibilidades culturales. Artistas inspirados en la fluorescencia podrían crear pinturas que brillan bajo lámparas ultravioletas, intentando imitar la complejidad cromática que estos insectos presentan de manera natural. La joyería artesanal podría incorporar fragmentos de exoesqueletos caídos, utilizándolos como gemas orgánicas luminiscentes. La medicina tradicional, por su parte, podría atribuir propiedades curativas a la luz emitida por estos seres, empleándolos en rituales nocturnos o creando ungüentos con extractos fluo\n",
      "\n",
      "\n",
      "Generated Answer: ¿Qué implicaciones tendría la existencia de insectos fluorescentes en el arte, la joyería y la medicina?\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import cohere\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Cohere API\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "# Function to chunk text manually (by character length)\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to get embeddings using Cohere's API\n",
    "def get_embedding(text):\n",
    "    input_type=\"search_query\"\n",
    "    response = co.embed(texts=[text],\n",
    "                         model=\"embed-english-v3.0\",  \n",
    "                         input_type=input_type,\n",
    "                         embedding_types=['float'])\n",
    "    return response.embeddings.float[0]\n",
    "\n",
    "# Function to generate response using Cohere's generation API\n",
    "def generate_response(context, query):\n",
    "    response = co.generate(\n",
    "        model=\"command-r-plus\",\n",
    "        prompt=f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
    "        max_tokens=512,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.generations[0].text.strip()\n",
    "\n",
    "\n",
    "with open(\"texto_muestra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    plain_text = f.read()\n",
    "\n",
    "chunks = chunk_text(plain_text, chunk_size=1000) ## CUIDADO CON EL NUMERO DE LLAMADAS\n",
    "\n",
    "# 3. Embed each chunk and create a FAISS index\n",
    "dimension = 1024  # Cohere embed-english-v3.0 dimension\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Convert embeddings to numpy array and add to FAISS\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "index.add(embeddings)\n",
    "\n",
    "# 4. Query and retrieve relevant chunks\n",
    "query = input(\"Introduce tu repuesta: \")\n",
    "query_embedding = np.array(get_embedding(query)).astype('float32').reshape(1, -1)\n",
    "\n",
    "# Perform search\n",
    "k = 1  # Top-1 most relevant chunk\n",
    "distances, indices = index.search(query_embedding, k=k)\n",
    "\n",
    "# Display the most relevant chunk\n",
    "retrieved_chunk = \"\"\n",
    "for i in range(k):\n",
    "    retrieved_chunk += chunks[indices[0][i]] + \"\\n\"\n",
    "\n",
    "print(\"\\nQuery:\", query)\n",
    "print(\"Retrieved chunk:\", retrieved_chunk)\n",
    "\n",
    "# 5. Generate response using the retrieved chunk as context\n",
    "generated_answer = generate_response(retrieved_chunk, query)\n",
    "print(\"\\nGenerated Answer:\", generated_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
