{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMERS 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" class=\"bg mq mr c\" width=\"1000\" height=\"563\" loading=\"eager\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1500/0*dwfAOAIZEfya11Nl\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"ab ca\"><div class=\"ch bg dx dy dz ea\"><p id=\"0cf5\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">We’ve been hearing a lot about Transformers and with good reason. They have taken the world of NLP by storm in the last few years. The Transformer is an architecture that uses Attention to significantly improve the performance of deep learning NLP translation models. It was first introduced in the paper <a class=\"af mx\" href=\"https://arxiv.org/abs/1706.03762\" rel=\"noopener ugc nofollow\" target=\"_blank\">Attention is all you need</a> and was quickly established as the leading architecture for most text data applications.</p><p id=\"6ab5\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Since then, numerous projects including Google’s BERT and OpenAI’s GPT series have built on this foundation and published performance results that handily beat existing state-of-the-art benchmarks.</p><p id=\"3b16\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Over a series of articles, I’ll go over the basics of Transformers, its architecture, and how it works internally. We will cover the Transformer functionality in a top-down manner. In later articles, we will look under the covers to understand the operation of the system in detail. We will also do a deep dive into the workings of the multi-head attention, which is the heart of the Transformer.</p><p id=\"39bd\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Here’s a quick summary of the previous and following articles in the series. My goal throughout will be to understand not just how something works but why it works that way.</p><ol class=\"\"><li id=\"51a3\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><strong class=\"na fh\">Overview of functionality — this article</strong> <em class=\"oa\">(How Transformers are used, and why they are better than RNNs. Components of the architecture, and behavior during Training and Inference)</em></li><li id=\"03f4\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34\">How it works</a> <em class=\"oa\">(Internal operation end-to-end. How data flows and what computations are performed, including matrix representations)</em></li><li id=\"fee8\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\">Multi-head Attention</a> <em class=\"oa\">(Inner workings of the Attention module throughout the Transformer)</em></li><li id=\"b569\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3\">Why Attention Boosts Performance</a> <em class=\"oa\">(Not just what Attention does but why it works so well. How does Attention capture the relationships between words in a sentence)</em></li></ol><p id=\"e8a7\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">And if you’re interested in NLP applications in general, I have some other articles you might like.</p><ol class=\"\"><li id=\"1057\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24\">Beam Search</a> <em class=\"oa\">(Algorithm commonly used by Speech-to-Text and NLP applications to enhance predictions)</em></li><li id=\"07d4\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\"><a class=\"af mx\" rel=\"noopener\" target=\"_blank\" href=\"/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b\">Bleu Score</a> (<em class=\"oa\">Bleu Score and Word Error Rate are two essential metrics for NLP models</em>)</li></ol><h1 id=\"fe4e\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">What is a Transformer</h1><p id=\"5469\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer architecture excels at handling text data which is inherently sequential. They take a text sequence as input and produce another text sequence as output. eg. to translate an input English sentence to Spanish.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md ph\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 1100w, https://miro.medium.com/v2/resize:fit:656/format:webp/1*XbPTo3Wi7q-HhwwhV4A9yA.png 656w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 328px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*XbPTo3Wi7q-HhwwhV4A9yA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XbPTo3Wi7q-HhwwhV4A9yA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XbPTo3Wi7q-HhwwhV4A9yA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XbPTo3Wi7q-HhwwhV4A9yA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XbPTo3Wi7q-HhwwhV4A9yA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XbPTo3Wi7q-HhwwhV4A9yA.png 1100w, https://miro.medium.com/v2/resize:fit:656/1*XbPTo3Wi7q-HhwwhV4A9yA.png 656w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 328px\"><img alt=\"\" class=\"bg mq mr c\" width=\"328\" height=\"344\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:492/1*XbPTo3Wi7q-HhwwhV4A9yA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"51b7\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">At its core, it contains a stack of Encoder layers and Decoder layers. To avoid confusion we will refer to the individual layer as an Encoder or a Decoder and will use Encoder stack or Decoder stack for a group of Encoder layers.</p><p id=\"8576\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Encoder stack and the Decoder stack each have their corresponding Embedding layers for their respective inputs. Finally, there is an Output layer to generate the final output.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pi\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1100w, https://miro.medium.com/v2/resize:fit:1040/format:webp/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1040w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1100w, https://miro.medium.com/v2/resize:fit:1040/1*XDtQ3C7XrtVuqTtrxr2UWQ.png 1040w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px\"><img alt=\"\" class=\"bg mq mr c\" width=\"520\" height=\"610\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:780/1*XDtQ3C7XrtVuqTtrxr2UWQ.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"c64b\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">All the Encoders are identical to one another. Similarly, all the Decoders are identical.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pj\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 1100w, https://miro.medium.com/v2/resize:fit:1098/format:webp/1*F7JlVjpmv-XAEeE9IPyzHA.png 1098w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 549px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*F7JlVjpmv-XAEeE9IPyzHA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*F7JlVjpmv-XAEeE9IPyzHA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*F7JlVjpmv-XAEeE9IPyzHA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*F7JlVjpmv-XAEeE9IPyzHA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*F7JlVjpmv-XAEeE9IPyzHA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*F7JlVjpmv-XAEeE9IPyzHA.png 1100w, https://miro.medium.com/v2/resize:fit:1098/1*F7JlVjpmv-XAEeE9IPyzHA.png 1098w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 549px\"><img alt=\"\" class=\"bg mq mr c\" width=\"549\" height=\"315\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:824/1*F7JlVjpmv-XAEeE9IPyzHA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><ul class=\"\"><li id=\"5a76\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The Encoder contains the all-important Self-attention layer that computes the relationship between different words in the sequence, as well as a Feed-forward layer.</li><li id=\"613d\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The Decoder contains the Self-attention layer and the Feed-forward layer, as well as a second Encoder-Decoder attention layer.</li><li id=\"925b\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">Each Encoder and Decoder has its own set of weights.</li></ul><p id=\"af90\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Encoder is a reusable module that is the defining component of all Transformer architectures. In addition to the above two layers, it also has Residual skip connections around both layers along with two LayerNorm layers.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pl\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*THykpgtL058A9EpkstnUJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*THykpgtL058A9EpkstnUJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*THykpgtL058A9EpkstnUJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*THykpgtL058A9EpkstnUJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*THykpgtL058A9EpkstnUJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*THykpgtL058A9EpkstnUJQ.png 1100w, https://miro.medium.com/v2/resize:fit:490/format:webp/1*THykpgtL058A9EpkstnUJQ.png 490w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 245px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*THykpgtL058A9EpkstnUJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*THykpgtL058A9EpkstnUJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*THykpgtL058A9EpkstnUJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*THykpgtL058A9EpkstnUJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*THykpgtL058A9EpkstnUJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*THykpgtL058A9EpkstnUJQ.png 1100w, https://miro.medium.com/v2/resize:fit:490/1*THykpgtL058A9EpkstnUJQ.png 490w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 245px\"><img alt=\"\" class=\"bg mq mr c\" width=\"245\" height=\"409\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:368/1*THykpgtL058A9EpkstnUJQ.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"306a\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">There are many variations of the Transformer architecture. Some Transformer architectures have no Decoder at all and rely only on the Encoder.</p><h1 id=\"27c9\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">What does Attention Do?</h1><p id=\"9f2d\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The key to the Transformer’s ground-breaking performance is its use of Attention.</p><p id=\"2e88\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\"><mark class=\"aed aee ao\">While processing a word, Attention enables the model to focus on other words in the input that are closely related to that word.</mark></p><p id=\"7d05\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">eg. ‘Ball’ is closely related to ‘blue’ and ‘holding’. On the other hand, ‘blue’ is not related to ‘boy’.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md pm\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 1100w, https://miro.medium.com/v2/resize:fit:634/format:webp/1*1ouB-xrMxPgqu721_zzsbA.png 634w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 317px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*1ouB-xrMxPgqu721_zzsbA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*1ouB-xrMxPgqu721_zzsbA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*1ouB-xrMxPgqu721_zzsbA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*1ouB-xrMxPgqu721_zzsbA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*1ouB-xrMxPgqu721_zzsbA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*1ouB-xrMxPgqu721_zzsbA.png 1100w, https://miro.medium.com/v2/resize:fit:634/1*1ouB-xrMxPgqu721_zzsbA.png 634w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 317px\"><img alt=\"\" class=\"bg mq mr c\" width=\"317\" height=\"91\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:476/1*1ouB-xrMxPgqu721_zzsbA.png\"></picture></div></figure><p id=\"1251\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer architecture uses self-attention by relating every word in the input sequence to every other word.</p><p id=\"47ee\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">eg. Consider two sentences:</p><ul class=\"\"><li id=\"88b2\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The <em class=\"oa\">cat</em> drank the milk because <strong class=\"na fh\">it</strong> was hungry.</li><li id=\"3b9d\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The cat drank the <em class=\"oa\">milk</em> because <strong class=\"na fh\">it</strong> was sweet.</li></ul><p id=\"a816\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In the first sentence, the word ‘it’ refers to ‘cat’, while in the second it refers to ‘milk. When the model processes the word ‘it’, self-attention gives the model more information about its meaning so that it can associate ‘it’ with the correct word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pn\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*pT0ZIWeoilLkz3e_1fVeYQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pT0ZIWeoilLkz3e_1fVeYQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pT0ZIWeoilLkz3e_1fVeYQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pT0ZIWeoilLkz3e_1fVeYQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pT0ZIWeoilLkz3e_1fVeYQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*pT0ZIWeoilLkz3e_1fVeYQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"389\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*pT0ZIWeoilLkz3e_1fVeYQ.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">Dark colors represent higher attention (Image by Author)</figcaption></figure><p id=\"e8d9\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">To enable it to handle more nuances about the intent and semantics of the sentence, Transformers include multiple attention scores for each word.</p><p id=\"1b7b\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">eg. While processing the word ‘it’, the first score highlights ‘cat’, while the second score highlights ‘hungry’. So when it decodes the word ‘it’, by translating it into a different language, for instance, it will incorporate some aspect of both ‘cat’ and ‘hungry’ into the translated word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div class=\"mc md po\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 1100w, https://miro.medium.com/v2/resize:fit:984/format:webp/1*s2hugsMP28aB2tJoYCq8uA.png 984w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 492px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*s2hugsMP28aB2tJoYCq8uA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*s2hugsMP28aB2tJoYCq8uA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*s2hugsMP28aB2tJoYCq8uA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*s2hugsMP28aB2tJoYCq8uA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*s2hugsMP28aB2tJoYCq8uA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*s2hugsMP28aB2tJoYCq8uA.png 1100w, https://miro.medium.com/v2/resize:fit:984/1*s2hugsMP28aB2tJoYCq8uA.png 984w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 492px\"><img alt=\"\" class=\"bg mq mr c\" width=\"492\" height=\"557\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:738/1*s2hugsMP28aB2tJoYCq8uA.png\"></picture></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"a5ce\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Training the Transformer</h1><p id=\"3982\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer works slightly differently during Training and while doing Inference.</p><p id=\"bdd4\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Let’s first look at the flow of data during Training. Training data consists of two parts:</p><ul class=\"\"><li id=\"8019\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The source or input sequence (eg. “You are welcome” in English, for a translation problem)</li><li id=\"6dc0\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The destination or target sequence (eg. “De nada” in Spanish)</li></ul><p id=\"73a9\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer’s goal is to learn how to output the target sequence, by using both the input and target sequence.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pp\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0g4qdq7Rt6QvDalFFAkL5g.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*0g4qdq7Rt6QvDalFFAkL5g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*0g4qdq7Rt6QvDalFFAkL5g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*0g4qdq7Rt6QvDalFFAkL5g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*0g4qdq7Rt6QvDalFFAkL5g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*0g4qdq7Rt6QvDalFFAkL5g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*0g4qdq7Rt6QvDalFFAkL5g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*0g4qdq7Rt6QvDalFFAkL5g.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"521\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*0g4qdq7Rt6QvDalFFAkL5g.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><p id=\"1443\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer processes the data like this:</p><ol class=\"\"><li id=\"e762\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The input sequence is converted into Embeddings (with Position Encoding) and fed to the Encoder.</li><li id=\"808b\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Encoders processes this and produces an encoded representation of the input sequence.</li><li id=\"fe17\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The target sequence is prepended with a start-of-sentence token, converted into Embeddings (with Position Encoding), and fed to the Decoder.</li><li id=\"5b25\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Decoders processes this along with the Encoder stack’s encoded representation to produce an encoded representation of the target sequence.</li><li id=\"bbd2\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The Output layer converts it into word probabilities and the final output sequence.</li><li id=\"51ef\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The Transformer’s Loss function compares this output sequence with the target sequence from the training data. This loss is used to generate gradients to train the Transformer during back-propagation.</li></ol><h1 id=\"96e2\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Inference</h1><p id=\"8b0f\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">During Inference, we have only the input sequence and don’t have the target sequence to pass as input to the Decoder. The goal of the Transformer is to produce the target sequence from the input sequence alone.</p><p id=\"dae1\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">So, like in a Seq2Seq model, we generate the output in a loop and feed the output sequence from the previous timestep to the Decoder in the next timestep until we come across an end-of-sentence token.</p><p id=\"e6d9\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The difference from the Seq2Seq model is that, at each timestep, we re-feed the entire output sequence generated thus far, rather than just the last word.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md pp\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-uvybwr8xULd3ug9ZwcSaQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*-uvybwr8xULd3ug9ZwcSaQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-uvybwr8xULd3ug9ZwcSaQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-uvybwr8xULd3ug9ZwcSaQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-uvybwr8xULd3ug9ZwcSaQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-uvybwr8xULd3ug9ZwcSaQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-uvybwr8xULd3ug9ZwcSaQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-uvybwr8xULd3ug9ZwcSaQ.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"521\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*-uvybwr8xULd3ug9ZwcSaQ.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">Inference flow, after first timestep (Image by Author)</figcaption></figure><p id=\"32ec\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The flow of data during Inference is:</p><ol class=\"\"><li id=\"72ee\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The input sequence is converted into Embeddings (with Position Encoding) and fed to the Encoder.</li><li id=\"ddc3\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Encoders processes this and produces an encoded representation of the input sequence.</li><li id=\"a545\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">Instead of the target sequence, we use an empty sequence with only a start-of-sentence token. This is converted into Embeddings (with Position Encoding) and fed to the Decoder.</li><li id=\"0834\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The stack of Decoders processes this along with the Encoder stack’s encoded representation to produce an encoded representation of the target sequence.</li><li id=\"bcb9\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">The Output layer converts it into word probabilities and produces an output sequence.</li><li id=\"d5f1\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">We take the last word of the output sequence as the predicted word. That word is now filled into the second position of our Decoder input sequence, which now contains a start-of-sentence token and the first word.</li><li id=\"a0cd\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt nx ny nz bj\" data-selectable-paragraph=\"\">Go back to step #3. As before, feed the new Decoder sequence into the model. Then take the second word of the output and append it to the Decoder sequence. Repeat this until it predicts an end-of-sentence token. Note that since the Encoder sequence does not change for each iteration, we do not have to repeat steps #1 and #2 each time (<em class=\"oa\">Thanks to Michal Kučírka for pointing this out</em>).</li></ol><h1 id=\"63d2\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">Teacher Forcing</h1><p id=\"7f8f\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">The approach of feeding the target sequence to the Decoder during training is known as Teacher Forcing. Why do we do this and what does that term mean?</p><p id=\"69eb\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">During training, we could have used the same approach that is used during inference. In other words, run the Transformer in a loop, take the last word from the output sequence, append it to the Decoder input and feed it to the Decoder for the next iteration. Finally, when the end-of-sentence token is predicted, the Loss function would compare the generated output sequence to the target sequence in order to train the network.</p><p id=\"0cbe\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Not only would this looping cause training to take much longer, but it also makes it harder to train the model. The model would have to predict the second word based on a potentially erroneous first predicted word, and so on.</p><p id=\"05a2\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Instead, by feeding the target sequence to the Decoder, we are giving it a hint, so to speak, just like a Teacher would. Even though it predicted an erroneous first word, it can instead use the correct first word to predict the second word so that those errors don’t keep compounding.</p><p id=\"2fdd\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">In addition, the Transformer is able to output all the words in parallel without looping, which greatly speeds up training.</p><h1 id=\"0e11\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">What are Transformers used for?</h1><p id=\"68a6\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">Transformers are very versatile and are used for most NLP tasks such as language models and text classification. They are frequently used in sequence-to-sequence models for applications such as Machine Translation, Text Summarization, Question-Answering, Named Entity Recognition, and Speech Recognition.</p><p id=\"a5f0\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">There are different flavors of the Transformer architecture for different problems. The basic Encoder Layer is used as a common building block for these architectures, with different application-specific ‘heads’ depending on the problem being solved.</p><h2 id=\"37d8\" class=\"pq oh ev be oi pr ps pt ol pu pv pw oo nh px py pz nl qa qb qc np qd qe qf fc bj\" data-selectable-paragraph=\"\">Transformer Classification architecture</h2><p id=\"32b4\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">A Sentiment Analysis application, for instance, would take a text document as input. A Classification head takes the Transformer’s output and generates predictions of the class labels such as a positive or negative sentiment.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md qg\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkqBjeTRZMRfOLiLqYV0TA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*tkqBjeTRZMRfOLiLqYV0TA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*tkqBjeTRZMRfOLiLqYV0TA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*tkqBjeTRZMRfOLiLqYV0TA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*tkqBjeTRZMRfOLiLqYV0TA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*tkqBjeTRZMRfOLiLqYV0TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*tkqBjeTRZMRfOLiLqYV0TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*tkqBjeTRZMRfOLiLqYV0TA.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"185\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*tkqBjeTRZMRfOLiLqYV0TA.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h2 id=\"7ba8\" class=\"pq oh ev be oi pr ps pt ol pu pv pw oo nh px py pz nl qa qb qc np qd qe qf fc bj\" data-selectable-paragraph=\"\">Transformer Language Model architecture</h2><p id=\"26a7\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">A Language Model architecture would take the initial part of an input sequence such as a text sentence as input, and generate new text by predicting sentences that would follow. A Language Model head takes the Transformer’s output and generates a probability for every word in the vocabulary. The highest probability word becomes the predicted output for the next word in the sentence.</p><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md qh\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*9HgXzK95-QTOQi5eyZ-S_Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*9HgXzK95-QTOQi5eyZ-S_Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*9HgXzK95-QTOQi5eyZ-S_Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*9HgXzK95-QTOQi5eyZ-S_Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*9HgXzK95-QTOQi5eyZ-S_Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*9HgXzK95-QTOQi5eyZ-S_Q.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"184\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*9HgXzK95-QTOQi5eyZ-S_Q.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><h1 id=\"c10b\" class=\"og oh ev be oi oj ok gc ol om on gf oo op oq or os ot ou ov ow ox oy oz pa pb bj\" data-selectable-paragraph=\"\">How are they better than RNNs?</h1><p id=\"9697\" class=\"pw-post-body-paragraph my mz ev na b ga pc nc nd gd pd nf ng nh pe nj nk nl pf nn no np pg nr ns nt eo bj\" data-selectable-paragraph=\"\">RNNs and their cousins, LSTMs and GRUs, were the de facto architecture for all NLP applications until Transformers came along and dethroned them.</p><p id=\"9f77\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">RNN-based sequence-to-sequence models performed well, and when the Attention mechanism was first introduced, it was used to enhance their performance.</p><p id=\"5ce8\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">However, they had two limitations:</p><ul class=\"\"><li id=\"5b30\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">It was challenging to deal with long-range dependencies between words that were spread far apart in a long sentence.</li><li id=\"283e\" class=\"my mz ev na b ga ob nc nd gd oc nf ng nu od nj nk nv oe nn no nw of nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">They process the input sequence sequentially one word at a time, which means that it cannot do the computation for time-step <em class=\"oa\">t</em> until it has completed the computation for time-step <em class=\"oa\">t — 1</em>. This slows down training and inference.</li></ul><p id=\"9b0a\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">As an aside, with CNNs, all of the outputs can be computed in parallel, which makes convolutions much faster. However, they also have limitations in dealing with long-range dependencies:</p><ul class=\"\"><li id=\"813f\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">In a convolutional layer, only parts of the image (or words if applied to text data) that are close enough to fit within the kernel size can interact with each other. For items that are further apart, you need a much deeper network with many layers.</li></ul><p id=\"0d58\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">The Transformer architecture addresses both of these limitations. It got rid of RNNs altogether and relied exclusively on the benefits of Attention.</p><ul class=\"\"><li id=\"6a4d\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">They process all the words in the sequence in parallel, thus greatly speeding up computation.</li></ul><figure class=\"mf mg mh mi mj lt mc md paragraph-image\"><div role=\"button\" tabindex=\"0\" class=\"mm mn hh mo bg mp\"><div class=\"mc md qi\"><picture><source srcset=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iygs9mQi4GbIJuc6fwBRKg.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\" type=\"image/webp\"><source data-testid=\"og\" srcset=\"https://miro.medium.com/v2/resize:fit:640/1*Iygs9mQi4GbIJuc6fwBRKg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Iygs9mQi4GbIJuc6fwBRKg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Iygs9mQi4GbIJuc6fwBRKg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Iygs9mQi4GbIJuc6fwBRKg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Iygs9mQi4GbIJuc6fwBRKg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Iygs9mQi4GbIJuc6fwBRKg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Iygs9mQi4GbIJuc6fwBRKg.png 1400w\" sizes=\"(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px\"><img alt=\"\" class=\"bg mq mr c\" width=\"700\" height=\"263\" loading=\"lazy\" role=\"presentation\" src=\"https://miro.medium.com/v2/resize:fit:1050/1*Iygs9mQi4GbIJuc6fwBRKg.png\"></picture></div></div><figcaption class=\"ms mt mu mc md mv mw be b bf z fd\" data-selectable-paragraph=\"\">(Image by Author)</figcaption></figure><ul class=\"\"><li id=\"2508\" class=\"my mz ev na b ga nb nc nd gd ne nf ng nu ni nj nk nv nm nn no nw nq nr ns nt pk ny nz bj\" data-selectable-paragraph=\"\">The distance between words in the input sequence does not matter. It is equally good at computing dependencies between adjacent words and words that are far apart.</li></ul><p id=\"f9e5\" class=\"pw-post-body-paragraph my mz ev na b ga nb nc nd gd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt eo bj\" data-selectable-paragraph=\"\">Now that we have a high-level idea of what a Transformer is, we can go deeper into its internal functionality in the next article to understand the details of how it works.</p></div></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
