{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efa0fce",
   "metadata": {},
   "source": [
    "### Pyspark GroupBy And Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c2038",
   "metadata": {},
   "source": [
    "#### INICIAMOS LA SESIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8f166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa06a829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-G810CJLM:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x13b8f410f40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed4d2f",
   "metadata": {},
   "source": [
    "#### GROUPBY Y AGGREGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9064d812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('data/test3.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8217134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3973bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Salary2', df_pyspark['Salary']*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d4002",
   "metadata": {},
   "source": [
    "GROUPBY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049dd05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|     Name|sum(salary)|sum(Salary2)|\n",
      "+---------+-----------+------------+\n",
      "|Sudhanshu|      35000|       70000|\n",
      "|    Sunny|      12000|       24000|\n",
      "|    Krish|      19000|       38000|\n",
      "|   Mahesh|       7000|       14000|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Por nombre y suma de salarios de cada persona\n",
    "\n",
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "105d0a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|     Name|max(salary)|max(Salary2)|\n",
      "+---------+-----------+------------+\n",
      "|Sudhanshu|      20000|       40000|\n",
      "|    Sunny|      10000|       20000|\n",
      "|    Krish|      10000|       20000|\n",
      "|   Mahesh|       4000|        8000|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0407bea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|     Name|min(salary)|min(Salary2)|\n",
      "+---------+-----------+------------+\n",
      "|Sudhanshu|       5000|       10000|\n",
      "|    Sunny|       2000|        4000|\n",
      "|    Krish|       4000|        8000|\n",
      "|   Mahesh|       3000|        6000|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f34b7168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+\n",
      "| Departments|sum(salary)|sum(Salary2)|\n",
      "+------------+-----------+------------+\n",
      "|         IOT|      15000|       30000|\n",
      "|    Big Data|      15000|       30000|\n",
      "|Data Science|      43000|       86000|\n",
      "+------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Por departamentos y suma de salarios\n",
    "\n",
    "df_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "180e8eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+\n",
      "| Departments|avg(salary)|avg(Salary2)|\n",
      "+------------+-----------+------------+\n",
      "|         IOT|     7500.0|     15000.0|\n",
      "|    Big Data|     3750.0|      7500.0|\n",
      "|Data Science|    10750.0|     21500.0|\n",
      "+------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Por departamento y salario medio\n",
    "\n",
    "df_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d33d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para contar el número de empleados por departamento\n",
    "\n",
    "df_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54ac41",
   "metadata": {},
   "source": [
    "AGGREGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7687158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|sum(Salary)|avg(Salary2)|\n",
      "+-----------+------------+\n",
      "|      73000|     14600.0|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum', \"Salary2\": \"mean\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f19890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint#configuration\n",
    "config = twint.Config()\n",
    "config.Search = \"@TheBridge_Tech\"\n",
    "\n",
    "config.Limit = 1000\n",
    "# config.Since = \"2022–01–01 00:00:00\"\n",
    "# config.Until = \"2022–07–12 00:00:00\"\n",
    "config.Store_json = True\n",
    "config.Output = \"custom_out.json\"#running search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3dd363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d8f62d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\josel\\Documents\\THISONE\\AQUI\\ENERO_22\\ENERO_22_DS\\4-Data_Engineering\\5-Big_Data\\5-groupby_aggfuncs.ipynb Celda 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josel/Documents/THISONE/AQUI/ENERO_22/ENERO_22_DS/4-Data_Engineering/5-Big_Data/5-groupby_aggfuncs.ipynb#ch0000018?line=0'>1</a>\u001b[0m twint\u001b[39m.\u001b[39;49mrun\u001b[39m.\u001b[39;49mSearch(config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\twint\\run.py:410\u001b[0m, in \u001b[0;36mSearch\u001b[1;34m(config, callback)\u001b[0m\n\u001b[0;32m    408\u001b[0m config\u001b[39m.\u001b[39mFollowers \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    409\u001b[0m config\u001b[39m.\u001b[39mProfile \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m run(config, callback)\n\u001b[0;32m    411\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mPandas_au:\n\u001b[0;32m    412\u001b[0m     storage\u001b[39m.\u001b[39mpanda\u001b[39m.\u001b[39m_autoget(\u001b[39m\"\u001b[39m\u001b[39mtweet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\twint\\run.py:329\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(config, callback)\u001b[0m\n\u001b[0;32m    325\u001b[0m     logme\u001b[39m.\u001b[39mexception(\n\u001b[0;32m    326\u001b[0m         \u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m:run:Unexpected exception occurred while attempting to get or create a new event loop.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    327\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m get_event_loop()\u001b[39m.\u001b[39;49mrun_until_complete(Twint(config)\u001b[39m.\u001b[39;49mmain(callback))\n",
      "File \u001b[1;32mc:\\Users\\josel\\anaconda3\\envs\\pyspark_env\\lib\\asyncio\\base_events.py:592\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \n\u001b[0;32m    583\u001b[0m \u001b[39mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m--> 592\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_running()\n\u001b[0;32m    594\u001b[0m new_task \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m futures\u001b[39m.\u001b[39misfuture(future)\n\u001b[0;32m    595\u001b[0m future \u001b[39m=\u001b[39m tasks\u001b[39m.\u001b[39mensure_future(future, loop\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\josel\\anaconda3\\envs\\pyspark_env\\lib\\asyncio\\base_events.py:552\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_running\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    551\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running():\n\u001b[1;32m--> 552\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThis event loop is already running\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    553\u001b[0m     \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    555\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCannot run the event loop while another loop is running\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "twint.run.Search(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc94f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyspark_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a3a425e162f4862738cbecfd5f787bbbf005b9ca551466c5a6c30f0a4f3fc04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
